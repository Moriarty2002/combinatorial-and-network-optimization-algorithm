{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9903b1c7",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ea507c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28210e55",
   "metadata": {},
   "source": [
    "---\n",
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6217fc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_video = 0\n",
    "num_endpoint = 0\n",
    "num_req = 0\n",
    "num_server = 0\n",
    "\n",
    "cache_capacity = 0\n",
    "video_size = []\n",
    "\n",
    "latency = defaultdict(dict)                         # [endpoint][cache/datacenter]\n",
    "reqs = defaultdict(lambda: defaultdict(int))        # [endpoint][video][num reqs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c85803ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = \"dataset/videos_worth_spreading.in\"\n",
    "dataset = \"dataset/me_at_the_zoo.in\"\n",
    "\n",
    "status = 0\n",
    "endpoint_index = 0\n",
    "num_connected_cache = 0\n",
    "with open(dataset, \"r\") as f:\n",
    "    for line_content in f:\n",
    "        line = line_content.split()\n",
    "\n",
    "        if status ==0:                                  # get counters\n",
    "            num_video = int(line[0])\n",
    "            num_endpoint = int(line[1])\n",
    "            num_req = int(line[2])\n",
    "            num_cache = int(line[3])\n",
    "            cache_capacity = int(line[4])\n",
    "            status = 1\n",
    "\n",
    "        elif status == 1:                               # get video dims\n",
    "            for size in line:\n",
    "                video_size.append(int(size))\n",
    "            status = 2\n",
    "\n",
    "        elif status == 2:                               # get datacenter latency and connected cache number\n",
    "            data_center_latency = int(line[0])\n",
    "            latency[endpoint_index][num_cache + 1] = data_center_latency\n",
    "            \n",
    "            num_connected_cache = int(line[1])\n",
    "            if not num_connected_cache:\n",
    "                endpoint_index = endpoint_index + 1\n",
    "                if endpoint_index == num_endpoint:\n",
    "                    status = 4\n",
    "            else:\n",
    "                status = 3\n",
    "        \n",
    "        elif status == 3:                                  # get cache latency\n",
    "            cache_index = int(line[0])\n",
    "            cache_latency = int(line[1])\n",
    "            latency[endpoint_index][cache_index] = cache_latency\n",
    "            \n",
    "            num_connected_cache = num_connected_cache - 1\n",
    "            if not num_connected_cache:\n",
    "                endpoint_index = endpoint_index + 1\n",
    "                if endpoint_index == num_endpoint:\n",
    "                    status = 4\n",
    "                else:\n",
    "                    status = 2\n",
    "        \n",
    "        elif status == 4:\n",
    "            video_index = int(line[0])\n",
    "            endpoint_index = int(line[1])\n",
    "            num_reqs = int(line[2])\n",
    "            reqs[endpoint_index][video_index] = num_reqs               # [endpoint][video][num reqs]            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "861cb172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340\n"
     ]
    }
   ],
   "source": [
    "print(reqs[4][27])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a922188b",
   "metadata": {},
   "source": [
    "---\n",
    "# Math model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47768109",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_index = range(num_endpoint)\n",
    "server_index = range(num_server + 1) # I've modelled datacenter as last server\n",
    "video_index = range(num_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bac327f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m y = model.addVars(server_index, video_index, vtype=gp.GRB.BINARY, name=\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# objective function\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m obj = \u001b[43mgp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquicksum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatency\u001b[49m\u001b[43m[\u001b[49m\u001b[43me\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m\u001b[43m*\u001b[49m\u001b[43mreqs\u001b[49m\u001b[43m[\u001b[49m\u001b[43me\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mv\u001b[49m\u001b[43m]\u001b[49m\u001b[43m*\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43mv\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mendpoint_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mserver_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvideo_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m model.setObjective(obj, GRB.MINIMIZE)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# constraints\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/gurobipy/_helpers.pyx:41\u001b[39m, in \u001b[36mgurobipy._helpers.quicksum\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m      5\u001b[39m y = model.addVars(server_index, video_index, vtype=gp.GRB.BINARY, name=\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# objective function\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m obj = gp.quicksum(\u001b[43mlatency\u001b[49m\u001b[43m[\u001b[49m\u001b[43me\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m*reqs[e][v]*x[e,s,v] \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m endpoint_index \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m server_index \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m video_index)\n\u001b[32m      9\u001b[39m model.setObjective(obj, GRB.MINIMIZE)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# constraints\u001b[39;00m\n",
      "\u001b[31mKeyError\u001b[39m: 0"
     ]
    }
   ],
   "source": [
    "model = gp.Model(\"YoutubeCache\")\n",
    "\n",
    "# decision vars\n",
    "x = model.addVars(endpoint_index, server_index, video_index, vtype=gp.GRB.BINARY, name=\"x\")\n",
    "y = model.addVars(server_index, video_index, vtype=gp.GRB.BINARY, name=\"y\")\n",
    "\n",
    "# objective function\n",
    "obj = gp.quicksum(latency[e][s]*reqs[e][v]*x[e,s,v] for e in endpoint_index for s in server_index for v in video_index)\n",
    "model.setObjective(obj, GRB.MINIMIZE)\n",
    "\n",
    "# constraints\n",
    "constr = gp.quicksum( (x[e,s,v] for e in endpoint_index) <= num_endpoint*y[s,v] for s in server_index for v in video_index ) \n",
    "model.addConstr(constr, name=\"if video v available on server s constraint\")\n",
    "\n",
    "constr = gp.quicksum() # TODO: check how to do this in a more efficient way (or use a new variable if not possible)\n",
    "model.addConstr(constr, name=\"CONSTRAINT EVERY REQUEST SATISFIED\")\n",
    "\n",
    "# TODO: check if is better to set datacenter capacity as inf instead of -1 approach\n",
    "constr = gp.quicksum( (video_size[v] * y[s,v] for v in video_index) <= cache_capacity for s in (server_index - 1)) # -1 because datacenter have all the video\n",
    "model.addConstr(constr, name=\"cache capacity constraint\")\n",
    "\n",
    "\n",
    "constr = gp.quicksum( y[num_cache,v] == 1 for v in video_index) # remember that cache servers are from 0 to n-1, n index is for datacenter\n",
    "model.addConstr(constr, name=\"Datacenter have all videos constraint\")\n",
    "\n",
    "constr = gp.quicksum( (x[e,s,v] for v in video_index) <= num_video*latency[e,s] for e in endpoint_index for s in server_index)\n",
    "model.addConstr(constr, name=\"if video v available on server s constraint\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
