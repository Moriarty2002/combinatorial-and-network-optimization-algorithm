{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9903b1c7",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea507c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import copy as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b59b1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMAL_SOLUTION = 0 # used to evaluate gap between math model and heuristics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28210e55",
   "metadata": {},
   "source": [
    "---\n",
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6217fc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_video = 0\n",
    "num_endpoint = 0\n",
    "num_req_descriptions = 0\n",
    "num_server = 0\n",
    "\n",
    "cache_capacity = 0\n",
    "video_size = []\n",
    "\n",
    "latency = defaultdict(lambda: defaultdict(int))     # [endpoint][cache/datacenter] = latency\n",
    "reqs = defaultdict(lambda: defaultdict(int))        # [endpoint][video] = num reqs\n",
    "\n",
    "# dataset = \"dataset/videos_worth_spreading.in\"\n",
    "dataset = \"dataset/me_at_the_zoo.in\"\n",
    "# dataset = \"dataset/custom.in\"\n",
    "# dataset = \"dataset/minimal.in\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85803ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "status = 0\n",
    "curr_endpoint_index = 0\n",
    "num_connected_cache = 0\n",
    "with open(dataset, \"r\") as f:\n",
    "    for line_content in f:\n",
    "        line = line_content.split()\n",
    "\n",
    "        if status ==0:                                  # get counters\n",
    "            num_video = int(line[0])\n",
    "            num_endpoint = int(line[1])\n",
    "            num_req_descriptions = int(line[2])\n",
    "            num_server = int(line[3])\n",
    "            cache_capacity = int(line[4])\n",
    "            status = 1\n",
    "\n",
    "        elif status == 1:                               # get video dims\n",
    "            for size in line:\n",
    "                video_size.append(int(size))\n",
    "            status = 2\n",
    "\n",
    "        elif status == 2:                               # get datacenter latency and connected cache number\n",
    "            data_center_latency = int(line[0])\n",
    "            latency[curr_endpoint_index][num_server] = data_center_latency\n",
    "            \n",
    "            num_connected_cache = int(line[1])\n",
    "            if not num_connected_cache:\n",
    "                curr_endpoint_index = curr_endpoint_index + 1\n",
    "                if curr_endpoint_index == num_endpoint:\n",
    "                    status = 4\n",
    "            else:\n",
    "                status = 3\n",
    "        \n",
    "        elif status == 3:                                  # get cache latency\n",
    "            cache_index = int(line[0])\n",
    "            cache_latency = int(line[1])\n",
    "            latency[curr_endpoint_index][cache_index] = cache_latency\n",
    "            \n",
    "            num_connected_cache = num_connected_cache - 1\n",
    "            if not num_connected_cache:\n",
    "                curr_endpoint_index = curr_endpoint_index + 1\n",
    "                if curr_endpoint_index == num_endpoint:\n",
    "                    status = 4\n",
    "                else:\n",
    "                    status = 2\n",
    "        \n",
    "        elif status == 4:                                   # take num requests\n",
    "            video_index = int(line[0])\n",
    "            curr_endpoint_index = int(line[1])\n",
    "            num_reqs = int(line[2])\n",
    "            reqs[curr_endpoint_index][video_index] = num_reqs                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fb2d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common indexes\n",
    "endpoint_index = range(num_endpoint)\n",
    "server_index = range(num_server + 1) # I've modelled datacenter as last server\n",
    "video_index = range(num_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861cb172",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"num video: {num_video}, num endpoints {num_endpoint}, req descriptions {num_req_descriptions}, num cache {num_server}, dim {cache_capacity}\")\n",
    "print(f\"video sized: {video_size}\")\n",
    "print(f\"latencies: {latency}\")\n",
    "print(f\"reqs: {reqs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a922188b",
   "metadata": {},
   "source": [
    "---\n",
    "# Math model for Guroby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac327f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gp.Model(\"YoutubeCache\")\n",
    "\n",
    "# DECISION VARS\n",
    "x = model.addVars(endpoint_index, server_index, video_index, vtype=gp.GRB.BINARY, name=\"x\")\n",
    "y = model.addVars(server_index, video_index, vtype=gp.GRB.BINARY, name=\"y\")\n",
    "\n",
    "# OBJECTIVE FUNCTION\n",
    "obj = gp.quicksum(latency[e][s]*reqs[e][v]*x[e,s,v] for e in endpoint_index for s in server_index for v in video_index)\n",
    "# the + y[s,v] it's used just to not let place useless video in cache server (but is not needed for this problem)\n",
    "# obj = gp.quicksum((latency[e][s]*reqs[e][v]*x[e,s,v] + y[s,v])for e in endpoint_index for s in server_index for v in video_index)\n",
    "model.setObjective(obj, GRB.MINIMIZE)\n",
    "\n",
    "\n",
    "# CONSTRAINTS\n",
    "constr = (gp.quicksum(x[e,s,v] for e in endpoint_index)  <= num_endpoint*y[s,v] for s in server_index for v in video_index )\n",
    "model.addConstrs(constr, name=\"if video v available on server s\")\n",
    "\n",
    "constr = ( gp.quicksum( x[e,s,v] for s in server_index ) == (1 if reqs[e][v] else 0) for e in endpoint_index for v in video_index ) # datacenter excluded \n",
    "model.addConstrs(constr, name=\"every request must be satisfied\")\n",
    "\n",
    "constr = ( gp.quicksum(video_size[v] * y[s,v] for v in video_index) <= cache_capacity for s in server_index[:-1] ) # -1 because datacenter have all the video\n",
    "model.addConstrs(constr, name=\"cache capacity\")\n",
    "\n",
    "\n",
    "constr = ( y[num_server,v] == 1 for v in video_index ) # cache servers are from 0 to s-1, s index (num_server) is for datacenter\n",
    "model.addConstrs(constr, name=\"Datacenter have all videos\")\n",
    "\n",
    "constr = ( gp.quicksum( x[e,s,v] for v in video_index ) <= (num_video*latency[e][s]) for e in endpoint_index for s in server_index[:-1] ) # -1 because datacenter have all the video\n",
    "model.addConstrs(constr, name=\"video v must be available on server s to be selected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d392c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the model\n",
    "model.optimize()\n",
    "\n",
    "# model.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16eebf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model output\n",
    "\n",
    "def print_full_output():\n",
    "    print(\"Optimal X [endpoint, server, video] values:\")\n",
    "    for e in endpoint_index:\n",
    "        for s in server_index:\n",
    "            for v in video_index:\n",
    "                print(f\"X[{e},{s},{v}] * {latency[e][s]} = {x[e,s,v]}\")\n",
    "    print(\"\\nOptimal Y [server, video] values:\")\n",
    "    for s in server_index:\n",
    "        for v in video_index:\n",
    "            print(f\"Y[{s},{v}] = {y[s,v]}\")\n",
    "\n",
    "def print_concise_output():\n",
    "    print(\"Optimal X [endpoint, server, video] values:\")\n",
    "    for e in endpoint_index:\n",
    "        for s in server_index:\n",
    "            for v in video_index:\n",
    "                if x[e,s,v].x:\n",
    "                    print(f\"X[{e},{s},{v}] * {latency[e][s]} = {x[e,s,v]}\")\n",
    "    print(\"\\nOptimal Y [server, video] values:\")\n",
    "    for s in server_index:\n",
    "        for v in video_index:\n",
    "            if y[s,v].x:\n",
    "                print(f\"Y[{s},{v}] = {y[s,v]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a6825c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results\n",
    "if model.status == gp.GRB.OPTIMAL:\n",
    "    print(\"\\nOptimization successful!\")\n",
    "    # print_full_output()\n",
    "    print_concise_output()\n",
    "    print(f\"\\nOptimal objective value: {model.objVal}\")\n",
    "    OPTIMAL_SOLUTION = model.ObjVal\n",
    "elif model.status == gp.GRB.INFEASIBLE:\n",
    "    print(\"Model is infeasible.\")\n",
    "elif model.status == gp.GRB.UNBOUNDED:\n",
    "    print(\"Model is unbounded.\")\n",
    "else:\n",
    "    print(f\"Optimization ended with status {model.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f13b29c",
   "metadata": {},
   "source": [
    "---\n",
    "# Heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c355ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Common\n",
    "def compute_obj_func(x):\n",
    "    return sum(latency[e][s]*reqs[e][v]*x[e,s,v] for e in endpoint_index for s in server_index for v in video_index)\n",
    "\n",
    "x_sol = []\n",
    "y_sol = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd09de88",
   "metadata": {},
   "source": [
    "## Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d547ffa",
   "metadata": {},
   "source": [
    "### 1. place video with highest request number in nearest cache when possible and place all videos for the endpoint in the order that we get\n",
    "order video by request number, and for every endpoint retrieven from this ordered list place all its requested videos in best available caches for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039aa21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "E_IND = 0\n",
    "V_IND = 1\n",
    "\n",
    "# Sort v indexes by descending value for endpoint e\n",
    "sorted_reqs = []\n",
    "for e in range(len(reqs)):\n",
    "    sorted_vs = sorted(\n",
    "        [v for v in reqs[e] if reqs[e][v] != 0],\n",
    "        key=lambda v: reqs[e][v],\n",
    "        reverse=True\n",
    "    )\n",
    "    sorted_reqs.extend((e, v) for v in sorted_vs)\n",
    "\n",
    "# Sort server s latency for endpoint e\n",
    "sorted_latency = defaultdict(list)\n",
    "for e in latency:\n",
    "    sorted_s = sorted(\n",
    "        [s for s in latency[e] if latency[e][s] != 0],\n",
    "        key=lambda s: latency[e][s]\n",
    "    )\n",
    "    sorted_latency[e] = sorted_s\n",
    "\n",
    "\n",
    "# use a list to keep current cache capacity (will be decreased every time a video is placed in cache)\n",
    "curr_capacity = [cache_capacity for _ in range(num_server)]\n",
    "curr_capacity.append(float('inf')) # datacenter doesn't have capacity\n",
    "\n",
    "# create vars (simil guroby, used numpy for efficiency)\n",
    "x = np.zeros((num_endpoint, (num_server+1), num_video)) \n",
    "y = np.zeros(((num_server+1), num_video)) \n",
    "y[num_server, :] = 1 # datacenter keep all the videos\n",
    "\n",
    "for req in sorted_reqs:\n",
    "    req_endpoint = req[E_IND]\n",
    "    req_video = req[V_IND]\n",
    "    req_video_size = video_size[req_video]\n",
    "\n",
    "    for curr_cache_index in sorted_latency[req_endpoint]:\n",
    "        if y[curr_cache_index, req_video]:\n",
    "            x[req_endpoint, curr_cache_index, req_video] = 1\n",
    "            break\n",
    "        else:\n",
    "            if curr_capacity[curr_cache_index] > req_video_size:\n",
    "                curr_capacity[curr_cache_index] -= req_video_size\n",
    "                y[curr_cache_index, req_video] = 1\n",
    "                x[req_endpoint, curr_cache_index, req_video] = 1\n",
    "                break\n",
    "\n",
    "# print(\"X\")\n",
    "# print(x)\n",
    "# print(\"Y\")\n",
    "# print(y)\n",
    "APPROX_RESULT = compute_obj_func(x)\n",
    "GAP = ( abs(OPTIMAL_SOLUTION - APPROX_RESULT) / OPTIMAL_SOLUTION ) * 100\n",
    "print(f\"APPROX RESULT: {APPROX_RESULT} - GAP: {GAP}% - OPTIMAL RESULT {OPTIMAL_SOLUTION}\")\n",
    "x_sol.append((x, APPROX_RESULT))\n",
    "y_sol.append((y, APPROX_RESULT))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d9e886",
   "metadata": {},
   "source": [
    "### 2. place video with highest request number in nearest cache + round robin (every iteration change endpoint)\n",
    "order video by request number, use a round robin schedulo to choose an endpoint retrieven from this ordered list and place its remaining requested video in best available cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0775a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "E_IND = 0\n",
    "V_IND = 1\n",
    "\n",
    "# Sort v indexes by descending value for endpoint e\n",
    "sorted_reqs = defaultdict(list)\n",
    "for e in range(len(reqs)):\n",
    "    sorted_vs = sorted(\n",
    "        [v for v in reqs[e] if reqs[e][v] != 0],\n",
    "        key=lambda v: reqs[e][v],\n",
    "        reverse=True\n",
    "    )\n",
    "    sorted_reqs[e] = sorted_vs\n",
    "\n",
    "# Sort server s latency for endpoint e\n",
    "sorted_latency = defaultdict(list)\n",
    "for e in latency:\n",
    "    sorted_s = sorted(\n",
    "        [s for s in latency[e] if latency[e][s] != 0],\n",
    "        key=lambda s: latency[e][s]\n",
    "    )\n",
    "    sorted_latency[e] = sorted_s\n",
    "\n",
    "\n",
    "# use a list to keep current cache capacity (will be decreased every time a video is placed in cache)\n",
    "curr_capacity = [cache_capacity for _ in range(num_server)]\n",
    "curr_capacity.append(float('inf')) # datacenter doesn't have capacity\n",
    "\n",
    "# create vars (simil guroby, used numpy for efficiency)\n",
    "x = np.zeros((num_endpoint, (num_server+1), num_video))\n",
    "y = np.zeros(((num_server+1), num_video))\n",
    "y[num_server, :] = 1 # datacenter keep all the videos\n",
    "\n",
    "Done = False\n",
    "endpoints_req_index = [0 for _ in server_index]\n",
    "while not Done:\n",
    "    Done = True\n",
    "    for curr_endpoint in endpoint_index:\n",
    "        curr_endpoint_req_index = endpoints_req_index[curr_endpoint]\n",
    "        \n",
    "        if curr_endpoint_req_index < len(sorted_reqs[curr_endpoint]):\n",
    "            Done = False # There could still be reqs not satisfied other than this\n",
    "            req_video = sorted_reqs[curr_endpoint][curr_endpoint_req_index]\n",
    "            req_video_size = video_size[req_video]\n",
    "\n",
    "            for curr_cache_index in sorted_latency[curr_endpoint]:\n",
    "                if y[curr_cache_index, req_video]:\n",
    "                    x[curr_endpoint, curr_cache_index, req_video] = 1\n",
    "                    break\n",
    "                else:\n",
    "                    if curr_capacity[curr_cache_index] > req_video_size:\n",
    "                        curr_capacity[curr_cache_index] -= req_video_size\n",
    "                        y[curr_cache_index, req_video] = 1\n",
    "                        x[curr_endpoint, curr_cache_index, req_video] = 1\n",
    "                        break\n",
    "                    \n",
    "        endpoints_req_index[curr_endpoint] += 1\n",
    "\n",
    "# print(\"X\")\n",
    "# print(x)\n",
    "# print(\"Y\")\n",
    "# print(y)\n",
    "APPROX_RESULT = compute_obj_func(x)\n",
    "GAP = ( abs(OPTIMAL_SOLUTION - APPROX_RESULT) / OPTIMAL_SOLUTION ) * 100\n",
    "print(f\"APPROX RESULT: {APPROX_RESULT} - GAP: {GAP}% - OPTIMAL RESULT {OPTIMAL_SOLUTION}\")\n",
    "x_sol.append((x, APPROX_RESULT))\n",
    "y_sol.append((y, APPROX_RESULT))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87489dbd",
   "metadata": {},
   "source": [
    "### 3. place video with highest request number in nearest cache when possible\n",
    "(only order by request number without considering the endpoints, pratically place cache video in the best server order by request number withouth reasoning on endpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e34ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "E_IND = 0\n",
    "V_IND = 1\n",
    "\n",
    "# Sort v indexes by descending value for endpoint e\n",
    "sorted_reqs = []\n",
    "sorted_reqs = sorted(\n",
    "    [(e, v) for e in range(len(reqs)) for v in reqs[e] if reqs[e][v] != 0],\n",
    "    key=lambda pair: reqs[pair[0]][pair[1]],\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "# Sort server s latency for endpoint e\n",
    "sorted_latency = defaultdict(list)\n",
    "for e in latency:\n",
    "    sorted_s = sorted(\n",
    "        [s for s in latency[e] if latency[e][s] != 0],\n",
    "        key=lambda s: latency[e][s]\n",
    "    )\n",
    "    sorted_latency[e] = sorted_s\n",
    "\n",
    "\n",
    "# use a list to keep current cache capacity (will be decreased every time a video is placed in cache)\n",
    "curr_capacity = [cache_capacity for _ in range(num_server)]\n",
    "curr_capacity.append(float('inf')) # datacenter doesn't have capacity\n",
    "\n",
    "# create vars (simil guroby, used numpy for efficiency)\n",
    "x = np.zeros((num_endpoint, (num_server+1), num_video))\n",
    "y = np.zeros(((num_server+1), num_video))\n",
    "y[num_server, :] = 1 # datacenter keep all the videos\n",
    "\n",
    "for req_endpoint,req_video in sorted_reqs:\n",
    "    req_video_size = video_size[req_video]\n",
    "\n",
    "    for curr_cache_index in sorted_latency[req_endpoint]:\n",
    "        if y[curr_cache_index, req_video]:\n",
    "            x[req_endpoint, curr_cache_index, req_video] = 1\n",
    "            break\n",
    "        else:\n",
    "            if curr_capacity[curr_cache_index] > req_video_size:\n",
    "                curr_capacity[curr_cache_index] -= req_video_size\n",
    "                y[curr_cache_index, req_video] = 1\n",
    "                x[req_endpoint, curr_cache_index, req_video] = 1\n",
    "                break\n",
    "\n",
    "# print(\"X\")\n",
    "# print(x)\n",
    "# print(\"Y\")\n",
    "# print(y)\n",
    "APPROX_RESULT = compute_obj_func(x)\n",
    "GAP = ( abs(OPTIMAL_SOLUTION - APPROX_RESULT) / OPTIMAL_SOLUTION ) * 100\n",
    "print(f\"APPROX RESULT: {APPROX_RESULT} - GAP: {GAP}% - OPTIMAL RESULT {OPTIMAL_SOLUTION}\")\n",
    "x_sol.append((x, APPROX_RESULT))\n",
    "y_sol.append((y, APPROX_RESULT))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0dd1d5",
   "metadata": {},
   "source": [
    "### 4. Place video ordered by popularity in cache with most connected endpoints for that video\n",
    "Order video by request number and place ordered video in the cache connected with most endpoints that have requested that specific video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa52075",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_req_count = [0 for _ in video_index]\n",
    "latency_sum = defaultdict(lambda: defaultdict(lambda: defaultdict(int))) # used to find best cache to place a video [server][video][latency sum / num endpoint / score]\n",
    "LATENCY_INDEX = 0\n",
    "NUM_ENDPOINT_CONNECTED_INDEX = 1\n",
    "SCORE_INDEX = 2\n",
    "\n",
    "for curr_endpoint_index, endpoint_reqs in reqs.items():\n",
    "    for curr_video_index, req_num in endpoint_reqs.items():\n",
    "        if req_num: # check if requests from endpoint for the video exists\n",
    "            video_req_count[curr_video_index] += req_num\n",
    "            for curr_server_index, lat in latency[curr_endpoint_index].items():\n",
    "                if curr_server_index != num_server and lat:\n",
    "                    latency_sum[curr_server_index][curr_video_index][LATENCY_INDEX] += lat\n",
    "                    latency_sum[curr_server_index][curr_video_index][NUM_ENDPOINT_CONNECTED_INDEX] += 1\n",
    "                    latency_sum[curr_server_index][curr_video_index][SCORE_INDEX] = (\n",
    "                        latency_sum[curr_server_index][curr_video_index][NUM_ENDPOINT_CONNECTED_INDEX] \n",
    "                        /\n",
    "                        latency_sum[curr_server_index][curr_video_index][LATENCY_INDEX]\n",
    "                    )\n",
    "            \n",
    "sorted_video_indexes = sorted(range(num_video), key=lambda i: video_req_count[i], reverse=True)\n",
    "\n",
    "# use a list to keep current cache capacity (will be decreased every time a video is placed in cache)\n",
    "curr_capacity = [cache_capacity for _ in range(num_server)]\n",
    "curr_capacity.append(float('inf')) # datacenter doesn't have capacity\n",
    "# print(curr_capacity)\n",
    "\n",
    "# create vars (simil guroby, used numpy for efficiency)\n",
    "x = np.zeros((num_endpoint, (num_server+1), num_video)) # e take v from s\n",
    "y = np.zeros(((num_server+1), num_video)) # v is in s\n",
    "y[num_server, :] = 1 # datacenter keep all the videos\n",
    "\n",
    "# Sort server s latency for endpoint e\n",
    "sorted_latency = defaultdict(list)\n",
    "for e in latency:\n",
    "    # Sort v indices by ascending value for this e\n",
    "    sorted_s = sorted(\n",
    "        [s for s in latency[e] if latency[e][s] != 0],\n",
    "        key=lambda s: latency[e][s]\n",
    "    )\n",
    "    sorted_latency[e] = sorted_s\n",
    "\n",
    "\n",
    "for curr_video_index in sorted_video_indexes:\n",
    "    # now sort caches to get the ones with most connected endpoints that have requested video curr_video_index\n",
    "    for curr_server_index in sorted(\n",
    "        [\n",
    "            i\n",
    "            for i in range(len(latency_sum))\n",
    "            if latency_sum[i][curr_video_index][NUM_ENDPOINT_CONNECTED_INDEX] != 0\n",
    "        ],\n",
    "        key=lambda i: latency_sum[i][curr_video_index][NUM_ENDPOINT_CONNECTED_INDEX],\n",
    "        reverse=True\n",
    "    ):\n",
    "        if not y[curr_server_index, curr_video_index]:\n",
    "            if curr_capacity[curr_server_index] > video_size[curr_video_index]:\n",
    "                curr_capacity[curr_server_index] -= video_size[curr_video_index]\n",
    "                y[curr_server_index, curr_video_index] = 1\n",
    "                break\n",
    "\n",
    "# iterate trough all reqs to check to what server the endpoint should request the video\n",
    "for req_endpoint,req_videos in reqs.items():    \n",
    "    for curr_video_index in req_videos:\n",
    "        if reqs[req_endpoint][curr_video_index]:\n",
    "            for curr_server_index in sorted_latency[req_endpoint]:\n",
    "                if y[curr_server_index, curr_video_index]:\n",
    "                    x[req_endpoint, curr_server_index, curr_video_index] = 1\n",
    "                    break\n",
    "\n",
    "# print(\"X\")\n",
    "# print(x)\n",
    "# print(\"Y\")\n",
    "# print(y)\n",
    "APPROX_RESULT = compute_obj_func(x)\n",
    "GAP = ( abs(OPTIMAL_SOLUTION - APPROX_RESULT) / OPTIMAL_SOLUTION ) * 100\n",
    "print(f\"APPROX RESULT: {APPROX_RESULT} - GAP: {GAP}% - OPTIMAL RESULT {OPTIMAL_SOLUTION}\")\n",
    "x_sol.append((x, APPROX_RESULT))\n",
    "y_sol.append((y, APPROX_RESULT))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f66908d",
   "metadata": {},
   "source": [
    "### 5. place video based on connected endpoint most requested videos\n",
    "for every caches, place their most requested video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702ccb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "E_IND = 0\n",
    "V_IND = 1\n",
    "\n",
    "# Sort v indexes by descending value for endpoint e\n",
    "sorted_reqs = []\n",
    "sorted_reqs = sorted(\n",
    "    [(e, v) for e in range(len(reqs)) for v in reqs[e] if reqs[e][v] != 0],\n",
    "    key=lambda pair: reqs[pair[0]][pair[1]],\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "# Sort server s latency for endpoint e\n",
    "sorted_latency = defaultdict(list)\n",
    "for e in latency:\n",
    "    sorted_s = sorted(\n",
    "        [s for s in latency[e] if latency[e][s] != 0],\n",
    "        key=lambda s: latency[e][s]\n",
    "    )\n",
    "    sorted_latency[e] = sorted_s\n",
    "\n",
    "\n",
    "# use a list to keep current cache capacity (will be decreased every time a video is placed in cache)\n",
    "curr_capacity = [cache_capacity for _ in range(num_server)]\n",
    "curr_capacity.append(float('inf')) # datacenter doesn't have capacity\n",
    "\n",
    "# create vars (simil guroby, used numpy for efficiency)\n",
    "x = np.zeros((num_endpoint, (num_server+1), num_video)) # e take v from s\n",
    "y = np.zeros(((num_server+1), num_video)) # v is in s\n",
    "y[num_server, :] = 1 # datacenter keep all the videos\n",
    "\n",
    "\n",
    "# get total possible reqs for any cache server from its connected endpoints\n",
    "server_total_reqs = defaultdict(lambda: defaultdict(int))\n",
    "for req_endpoint,req_videos in reqs.items():\n",
    "    for curr_server_index, lat in latency[req_endpoint].items():\n",
    "                if curr_server_index != num_server and lat:\n",
    "                    for curr_video_index in req_videos.keys():\n",
    "                        server_total_reqs[curr_server_index][curr_video_index] += req_videos[curr_video_index]\n",
    "\n",
    "\n",
    "# place video in caches based on request counts\n",
    "for curr_server_index in server_index[:-1]:\n",
    "    sorted_indexes_only = [\n",
    "        video_index\n",
    "        for video_index, count in sorted(\n",
    "            server_total_reqs[curr_server_index].items(),\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )\n",
    "        if count > 0\n",
    "    ]\n",
    "    for curr_video_index in sorted_indexes_only:\n",
    "        if not y[curr_server_index, curr_video_index] and curr_capacity[curr_server_index] > video_size[curr_video_index]:\n",
    "            curr_capacity[curr_server_index] -= video_size[curr_video_index]\n",
    "            y[curr_server_index, curr_video_index] = 1\n",
    "\n",
    "\n",
    "# iterate trough all reqs to check to what server the endpoint should request the video\n",
    "for req_endpoint,req_videos in reqs.items():    \n",
    "    for curr_video_index in req_videos:\n",
    "        if reqs[req_endpoint][curr_video_index]:\n",
    "            for curr_server_index in sorted_latency[req_endpoint]:\n",
    "                if y[curr_server_index, curr_video_index]:\n",
    "                    x[req_endpoint, curr_server_index, curr_video_index] = 1\n",
    "                    break\n",
    "\n",
    "# print(\"X\")\n",
    "# print(x)\n",
    "# print(\"Y\")\n",
    "# print(y)\n",
    "APPROX_RESULT = compute_obj_func(x)\n",
    "GAP = ( abs(OPTIMAL_SOLUTION - APPROX_RESULT) / OPTIMAL_SOLUTION ) * 100\n",
    "print(f\"APPROX RESULT: {APPROX_RESULT} - GAP: {GAP}% - OPTIMAL RESULT {OPTIMAL_SOLUTION}\")\n",
    "x_sol.append((x, APPROX_RESULT))\n",
    "y_sol.append((y, APPROX_RESULT))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e63615d",
   "metadata": {},
   "source": [
    "## Local Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab0c175",
   "metadata": {},
   "source": [
    "### tabu search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ff2d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_x(y):\n",
    "    x = np.zeros((num_endpoint, (num_server+1), num_video))\n",
    "\n",
    "    # Sort server s latency for endpoint e\n",
    "    sorted_latency = defaultdict(list)\n",
    "    for e in latency:\n",
    "        sorted_s = sorted(\n",
    "            [s for s in latency[e] if latency[e][s] != 0],\n",
    "            key=lambda s: latency[e][s]\n",
    "        )\n",
    "        sorted_latency[e] = sorted_s\n",
    "    \n",
    "    for curr_endpoint_index in endpoint_index:\n",
    "        for curr_video_index in reqs[curr_endpoint_index]:\n",
    "            best_server_index = num_server  # default to datacenter\n",
    "            \n",
    "            for curr_server_index in sorted_latency[curr_endpoint_index]:                    \n",
    "                if y[curr_server_index][curr_video_index]:\n",
    "                    best_server_index = curr_server_index\n",
    "                    break\n",
    "\n",
    "            x[curr_endpoint_index][best_server_index][curr_video_index] = 1\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5369a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabu_search_toggle(x, y, num_iters=1000, tabu_list_dim=10, max_no_improve=100):\n",
    "    tabu_list = deque(maxlen=tabu_list_dim)\n",
    "    intensification_list = []\n",
    "    \n",
    "    # Start with initial solution\n",
    "    best_x = np.copy(x)\n",
    "    best_y = np.copy(y)\n",
    "    best_obj_val = compute_obj_func(best_x)\n",
    "    no_improve_count = 0  \n",
    "    \n",
    "    for iteration in range(num_iters):\n",
    "        neighborhood = []\n",
    "\n",
    "        # Generate neighbor solutions\n",
    "        for curr_server_index in server_index[:-1]:  # Only cache servers\n",
    "            for curr_video_index in video_index:\n",
    "                move = (curr_server_index, curr_video_index)\n",
    "                if move in tabu_list:\n",
    "                    continue\n",
    "\n",
    "                # Try toggling video v in cache s\n",
    "                new_y = np.copy(y)\n",
    "                new_y[curr_server_index][curr_video_index] = 1 - new_y[curr_server_index][curr_video_index]\n",
    "\n",
    "                # Check cache capacity constraint\n",
    "                curr_video_size = sum(video_size[v] for v in video_index if new_y[curr_server_index][v])\n",
    "                if curr_video_size > cache_capacity:\n",
    "                    continue\n",
    "\n",
    "                # Generate new x according to new y\n",
    "                new_x = get_best_x(new_y)\n",
    "\n",
    "                obj_val = compute_obj_func(new_x)\n",
    "\n",
    "                neighborhood.append((obj_val, new_x, new_y, move))\n",
    "\n",
    "        if not neighborhood:\n",
    "            break\n",
    "\n",
    "        # Choose best neighbor\n",
    "        neighborhood.sort(key=lambda tup: tup[0])  # Sort by delay\n",
    "        obj_val, new_x, new_y, move = neighborhood[0]\n",
    "\n",
    "        if obj_val < best_obj_val:\n",
    "            best_obj_val = obj_val\n",
    "            best_x = new_x\n",
    "            best_y = new_y\n",
    "            intensification_list.append((best_obj_val, best_x, best_y))\n",
    "            intensification_list = sorted(intensification_list)[:10]  # keep top 10\n",
    "            no_improve_count = 0 # reset no consecutive improvement counter\n",
    "        else:\n",
    "            no_improve_count += 1\n",
    "            \n",
    "        if no_improve_count >= max_no_improve:\n",
    "            # print(f\"Early stopping after {no_improve_count} iterations without improvement.\")\n",
    "            break\n",
    "        \n",
    "        # Update current solution\n",
    "        x = new_x\n",
    "        y = new_y\n",
    "\n",
    "        # Update tabu list\n",
    "        tabu_list.append(move)\n",
    "        \n",
    "        # Random intensification\n",
    "        if iteration % 70 == 0 and intensification_list:\n",
    "            _, best_x, best_y = random.choice(intensification_list)\n",
    "            x, y = best_x.copy(), best_y.copy()\n",
    "\n",
    "    return best_x, best_y, best_obj_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136e156b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabu_search_add(x, y, num_iters=1000, tabu_list_dim=10, max_no_improve=100):\n",
    "    tabu_list = deque(maxlen=tabu_list_dim)\n",
    "    intensification_list = []\n",
    "    \n",
    "    # Start with initial solution\n",
    "    best_x = np.copy(x)\n",
    "    best_y = np.copy(y)\n",
    "    best_obj_val = compute_obj_func(best_x)\n",
    "    no_improve_count = 0  \n",
    "    \n",
    "    for iteration in range(num_iters):\n",
    "        neighborhood = []\n",
    "\n",
    "        # Generate neighbor solutions\n",
    "        for curr_server_index in server_index[:-1]:  # Only cache servers\n",
    "            for curr_video_index in video_index:\n",
    "                move = (curr_server_index, curr_video_index)\n",
    "                if move in tabu_list:\n",
    "                    continue\n",
    "\n",
    "                # Try toggling video v in cache s\n",
    "                new_y = np.copy(y)\n",
    "                \n",
    "                # Try adding current video in current server if not already present\n",
    "                if new_y[curr_server_index][curr_video_index]:\n",
    "                    continue\n",
    "                new_y[curr_server_index][curr_video_index] = 1 \n",
    "\n",
    "                # Check cache capacity constraint\n",
    "                curr_video_size = sum(video_size[v] for v in video_index if new_y[curr_server_index][v])\n",
    "                if curr_video_size > cache_capacity:\n",
    "                    continue\n",
    "\n",
    "                # Generate new x according to new y\n",
    "                new_x = get_best_x(new_y)\n",
    "\n",
    "                obj_val = compute_obj_func(new_x)\n",
    "\n",
    "                neighborhood.append((obj_val, new_x, new_y, move))\n",
    "\n",
    "        if not neighborhood:\n",
    "            break\n",
    "\n",
    "        # Choose best neighbor\n",
    "        neighborhood.sort(key=lambda tup: tup[0])  # Sort by delay\n",
    "        obj_val, new_x, new_y, move = neighborhood[0]\n",
    "\n",
    "        if obj_val < best_obj_val:\n",
    "            best_obj_val = obj_val\n",
    "            best_x = new_x\n",
    "            best_y = new_y\n",
    "            intensification_list.append((best_obj_val, best_x, best_y))\n",
    "            intensification_list = sorted(intensification_list)[:10]  # keep top 10\n",
    "            no_improve_count = 0 # reset no consecutive improvement counter\n",
    "        else:\n",
    "            no_improve_count += 1\n",
    "            \n",
    "        if no_improve_count >= max_no_improve:\n",
    "            # print(f\"Early stopping after {no_improve_count} iterations without improvement.\")\n",
    "            break\n",
    "\n",
    "        # Update current solution\n",
    "        x = new_x\n",
    "        y = new_y\n",
    "\n",
    "        # Update tabu list\n",
    "        tabu_list.append(move)\n",
    "        \n",
    "        # Random intensification\n",
    "        if iteration % 70 == 0 and intensification_list:\n",
    "            _, best_x, best_y = random.choice(intensification_list)\n",
    "            x, y = best_x.copy(), best_y.copy()\n",
    "\n",
    "    return best_x, best_y, best_obj_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24d4971",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"best sol: {OPTIMAL_SOLUTION}\")\n",
    "tabu_list_dim = np.floor(np.sqrt(num_req_descriptions))\n",
    "\n",
    "for sol_index in range(len(x_sol)):\n",
    "    default_obj_val = compute_obj_func(x_sol[sol_index][0])\n",
    "    x_tabu_toggle, y_tabu_toggle, obj_tabu_toggle = tabu_search_toggle(x_sol[sol_index][0], y_sol[sol_index][0], num_iters=400, tabu_list_dim=int(tabu_list_dim))\n",
    "    print(f\"old solution: {default_obj_val}, tabu sol: {obj_tabu_toggle}\")\n",
    "    \n",
    "    x_tabu_add, y_tabu_add, obj_tabu_add = tabu_search_add(x_sol[sol_index][0], y_sol[sol_index][0], num_iters=400, tabu_list_dim=int(tabu_list_dim))\n",
    "    print(f\"old solution: {default_obj_val}, tabu sol: {obj_tabu_add}\")\n",
    "    \n",
    "    if obj_tabu_toggle < obj_tabu_add:\n",
    "        if obj_tabu_toggle < default_obj_val:\n",
    "            x_sol.append((x_tabu_toggle, obj_tabu_toggle))\n",
    "            y_sol.append((y_tabu_toggle, obj_tabu_toggle))\n",
    "    else:\n",
    "        if obj_tabu_add < default_obj_val:\n",
    "            x_sol.append((x_tabu_add, obj_tabu_add))\n",
    "            y_sol.append((y_tabu_add, obj_tabu_add))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7701ef4",
   "metadata": {},
   "source": [
    "## Genetic algorithm\n",
    "\n",
    "#### fitness: minimize delay ( compute_obj_func(x) )\n",
    "#### initial population: heuristics solutions (x_sol & y_sol)\n",
    "#### randomization: montecarlo simulation\n",
    "#### crossover & mutation: on y (update x conseguently get_best_x(y) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c0566772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen 0: Best delay = 6347627.0\n",
      "Gen 1: Best delay = 6347627.0\n",
      "Gen 2: Best delay = 6188347.0\n",
      "Gen 3: Best delay = 6012783.0\n",
      "Gen 4: Best delay = 6012783.0\n",
      "Gen 5: Best delay = 6012783.0\n",
      "Gen 6: Best delay = 5867186.0\n",
      "Gen 7: Best delay = 5867186.0\n",
      "Gen 8: Best delay = 5867186.0\n",
      "Gen 9: Best delay = 5867186.0\n",
      "Gen 10: Best delay = 5867186.0\n",
      "Gen 11: Best delay = 5867186.0\n",
      "Gen 12: Best delay = 5867186.0\n",
      "Gen 13: Best delay = 5867186.0\n",
      "Gen 14: Best delay = 5867186.0\n",
      "Gen 15: Best delay = 5696036.0\n",
      "Gen 16: Best delay = 5696036.0\n",
      "Gen 17: Best delay = 5696036.0\n",
      "Gen 18: Best delay = 5696036.0\n",
      "Gen 19: Best delay = 5696036.0\n",
      "Gen 20: Best delay = 5696036.0\n",
      "Gen 21: Best delay = 5696036.0\n",
      "Gen 22: Best delay = 5696036.0\n",
      "Gen 23: Best delay = 5696036.0\n",
      "Gen 24: Best delay = 5696036.0\n",
      "Gen 25: Best delay = 5696036.0\n",
      "Gen 26: Best delay = 5518812.0\n",
      "Gen 27: Best delay = 5518812.0\n",
      "Gen 28: Best delay = 5518812.0\n",
      "Gen 29: Best delay = 5518812.0\n",
      "(array([[[1., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 1., 0., ..., 0., 0., 1.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 1., ..., 1., 1., 0.]],\n",
      "\n",
      "       [[1., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 1., 0., ..., 0., 0., 1.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 1., ..., 1., 1., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 1., 0., ..., 0., 0., 1.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 1., ..., 1., 1., 0.]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [1., 1., 0., ..., 1., 1., 1.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 1., 0., ..., 1., 1., 1.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [1., 1., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 1., ..., 1., 1., 1.]]]), array([[1., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 1., 0., ..., 0., 0., 1.],\n",
      "       [0., 1., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [1., 0., 1., ..., 0., 0., 0.],\n",
      "       [1., 1., 0., ..., 0., 0., 0.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.]]), np.float64(5518812.0))\n"
     ]
    }
   ],
   "source": [
    "X_IND = 0\n",
    "Y_IND = 1\n",
    "OBJ_IND = 2\n",
    "\n",
    "NUM_GEN = 30\n",
    "NUM_CROSSOVER_TRIAL = 30\n",
    "NUM_DEAD = 3\n",
    "MUT_RATE = 0.001\n",
    "\n",
    "def is_feasible(y):\n",
    "    for curr_server_index in server_index[:-1]:\n",
    "        # calculate total used space by video placed on cache curr_server_index\n",
    "        curr_tot_size = sum(video_size[curr_video_index] for curr_video_index in video_index if y[curr_server_index, curr_video_index])\n",
    "        if curr_tot_size > cache_capacity:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def montecarlo_roulette_individual_selection(current_population, reverse=False):\n",
    "    # Reverse = True: dead selection - Reverse = False: parent selection\n",
    "    if reverse:\n",
    "        fitness_list = [ curr_individual[OBJ_IND] for curr_individual in current_population ]\n",
    "    else:\n",
    "        # 1 / obj func because less delay will have bigger size\n",
    "        fitness_list = [ (1/curr_individual[OBJ_IND]) for curr_individual in current_population ]\n",
    "        \n",
    "    fitness_tot = sum(fitness_list)\n",
    "    \n",
    "    spin = random.uniform(0, fitness_tot)\n",
    "    \n",
    "    curr_fit = 0\n",
    "    for curr_individual_index in range(len(current_population)):\n",
    "        curr_fit += fitness_list[curr_individual_index]\n",
    "        if spin < curr_fit:\n",
    "            \n",
    "            if reverse:\n",
    "                return curr_individual_index\n",
    "            \n",
    "            return current_population[curr_individual_index]\n",
    "\n",
    "def crossover(parent1, parent2):\n",
    "    # we need to clone parent or python will reference to them (and if we do mutation we'll do it also on parents)\n",
    "    y1 = parent1[Y_IND].copy()\n",
    "    y2 = parent2[Y_IND].copy()\n",
    "    \n",
    "    childs_y = []\n",
    "    childs = []\n",
    "    \n",
    "    # Monosplit crossover\n",
    "    split = np.random.randint(1, num_server)\n",
    "    # take y cache configuration [0 to split] from parent 1 and remaining cache configuration [split to num_server+1] from parent2 \n",
    "    childs_y.append( np.vstack([y1[:split, :], y2[split:, :]]) )\n",
    "    childs_y.append( np.vstack([y1[:split, :], y2[split:, :]]) )\n",
    "    \n",
    "    # kill unfeasible child (spartan way)\n",
    "    for child_y in childs_y:\n",
    "        child_x = get_best_x(child_y)\n",
    "        childs.append((child_x, child_y, compute_obj_func(child_x)))\n",
    "    \n",
    "    return childs\n",
    "\n",
    "def mutate(childs):\n",
    "\n",
    "    childs_mutated = []\n",
    "    \n",
    "    for child in childs:\n",
    "        child_mutated = cp.deepcopy(child)\n",
    "        \n",
    "        for curr_server_index in server_index[:-1]:\n",
    "            for curr_video_index in video_index:\n",
    "                \n",
    "                # mutation is a toggle in y matrix\n",
    "                if random.random() < MUT_RATE:\n",
    "                    child_mutated[Y_IND][curr_server_index][curr_video_index] = 1 - child_mutated[Y_IND][curr_server_index][curr_video_index] \n",
    "        \n",
    "        # check if mutated child is feasible, otherwise keep unmutated one\n",
    "        if is_feasible(child_mutated[Y_IND]):\n",
    "            childs_mutated.append(child_mutated)                    \n",
    "        else:\n",
    "            childs_mutated.append(child)                    \n",
    "            \n",
    "    return childs_mutated\n",
    "\n",
    "def run_ga():\n",
    "    # an individual is (x, y, objective value)\n",
    "    population = [(x_sol[curr_individual_index][0], y_sol[curr_individual_index][0], x_sol[curr_individual_index][1]) for curr_individual_index in range(len(y_sol))]\n",
    "    # print(f\"Starting population: {population}\")\n",
    "    \n",
    "    for current_gen in range(NUM_GEN):\n",
    "        if population:\n",
    "            for _ in range(NUM_CROSSOVER_TRIAL): \n",
    "                parent_1 = montecarlo_roulette_individual_selection(population)\n",
    "                parent_2 = montecarlo_roulette_individual_selection(population)\n",
    "                childs = crossover(parent_1, parent_2)\n",
    "                childs = mutate(childs)\n",
    "                if childs:\n",
    "                    population.extend(childs)\n",
    "            \n",
    "            # remove an individual every iteration\n",
    "            # for _ in range(NUM_DEAD):\n",
    "            dead = montecarlo_roulette_individual_selection(population, reverse=True)\n",
    "            tomb = population.pop(dead)\n",
    "                \n",
    "            if population:\n",
    "                # print(f\"Death individual: {dead} - {tomb}\")\n",
    "                best = min(population, key=lambda individual: individual[OBJ_IND])\n",
    "                print(f\"Gen {current_gen}: Best delay = {best[OBJ_IND]}\")\n",
    "        else:\n",
    "            return \"GENOCIDE\"\n",
    "        \n",
    "        \n",
    "    return min(population, key=lambda individual: individual[OBJ_IND])\n",
    "\n",
    "sol = run_ga()\n",
    "print(f\"{sol}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
