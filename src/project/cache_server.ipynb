{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9903b1c7",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3ea507c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "from collections import defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4b59b1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMAL_SOLUTION = 0 # used to evaluate gap between math model and heuristics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28210e55",
   "metadata": {},
   "source": [
    "---\n",
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6217fc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_video = 0\n",
    "num_endpoint = 0\n",
    "num_req_descriptions = 0\n",
    "num_server = 0\n",
    "\n",
    "cache_capacity = 0\n",
    "video_size = []\n",
    "\n",
    "latency = defaultdict(lambda: defaultdict(int))                         # [endpoint][cache/datacenter] = latency\n",
    "reqs = defaultdict(lambda: defaultdict(int))        # [endpoint][video] = num reqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c85803ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = \"dataset/videos_worth_spreading.in\"\n",
    "# dataset = \"dataset/me_at_the_zoo.in\"\n",
    "dataset = \"dataset/custom.in\"\n",
    "# dataset = \"dataset/minimal.in\"\n",
    "\n",
    "status = 0\n",
    "endpoint_index = 0\n",
    "num_connected_cache = 0\n",
    "with open(dataset, \"r\") as f:\n",
    "    for line_content in f:\n",
    "        line = line_content.split()\n",
    "\n",
    "        if status ==0:                                  # get counters\n",
    "            num_video = int(line[0])\n",
    "            num_endpoint = int(line[1])\n",
    "            num_req_descriptions = int(line[2])\n",
    "            num_server = int(line[3])\n",
    "            cache_capacity = int(line[4])\n",
    "            status = 1\n",
    "\n",
    "        elif status == 1:                               # get video dims\n",
    "            for size in line:\n",
    "                video_size.append(int(size))\n",
    "            status = 2\n",
    "\n",
    "        elif status == 2:                               # get datacenter latency and connected cache number\n",
    "            data_center_latency = int(line[0])\n",
    "            latency[endpoint_index][num_server] = data_center_latency\n",
    "            \n",
    "            num_connected_cache = int(line[1])\n",
    "            if not num_connected_cache:\n",
    "                endpoint_index = endpoint_index + 1\n",
    "                if endpoint_index == num_endpoint:\n",
    "                    status = 4\n",
    "            else:\n",
    "                status = 3\n",
    "        \n",
    "        elif status == 3:                                  # get cache latency\n",
    "            cache_index = int(line[0])\n",
    "            cache_latency = int(line[1])\n",
    "            latency[endpoint_index][cache_index] = cache_latency\n",
    "            \n",
    "            num_connected_cache = num_connected_cache - 1\n",
    "            if not num_connected_cache:\n",
    "                endpoint_index = endpoint_index + 1\n",
    "                if endpoint_index == num_endpoint:\n",
    "                    status = 4\n",
    "                else:\n",
    "                    status = 2\n",
    "        \n",
    "        elif status == 4:                                   # take num requests\n",
    "            video_index = int(line[0])\n",
    "            endpoint_index = int(line[1])\n",
    "            num_reqs = int(line[2])\n",
    "            reqs[endpoint_index][video_index] = num_reqs                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "861cb172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num video: 5, num endpoints 2, req descriptions 4, num cache 3, dim 100\n",
      "video sized: [50, 50, 80, 30, 110]\n",
      "latencies: defaultdict(<function <lambda> at 0x7febe164b920>, {0: defaultdict(<class 'int'>, {3: 1000, 0: 100, 2: 200, 1: 300}), 1: defaultdict(<class 'int'>, {3: 500})})\n",
      "reqs: defaultdict(<function <lambda> at 0x7febf2e75f80>, {0: defaultdict(<class 'int'>, {3: 1500, 4: 500, 1: 1000}), 1: defaultdict(<class 'int'>, {0: 1000})})\n"
     ]
    }
   ],
   "source": [
    "print(f\"num video: {num_video}, num endpoints {num_endpoint}, req descriptions {num_req_descriptions}, num cache {num_server}, dim {cache_capacity}\")\n",
    "print(f\"video sized: {video_size}\")\n",
    "print(f\"latencies: {latency}\")\n",
    "print(f\"reqs: {reqs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a922188b",
   "metadata": {},
   "source": [
    "---\n",
    "# Math model for Guroby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47768109",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_index = range(num_endpoint)\n",
    "server_index = range(num_server + 1) # I've modelled datacenter as last server\n",
    "video_index = range(num_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bac327f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): <gurobi.Constr *Awaiting Model Update*>,\n",
       " (0, 1): <gurobi.Constr *Awaiting Model Update*>,\n",
       " (0, 2): <gurobi.Constr *Awaiting Model Update*>,\n",
       " (1, 0): <gurobi.Constr *Awaiting Model Update*>,\n",
       " (1, 1): <gurobi.Constr *Awaiting Model Update*>,\n",
       " (1, 2): <gurobi.Constr *Awaiting Model Update*>}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gp.Model(\"YoutubeCache\")\n",
    "\n",
    "# decision vars\n",
    "x = model.addVars(endpoint_index, server_index, video_index, vtype=gp.GRB.BINARY, name=\"x\")\n",
    "y = model.addVars(server_index, video_index, vtype=gp.GRB.BINARY, name=\"y\")\n",
    "\n",
    "# objective function\n",
    "# obj = gp.quicksum(latency[e][s]*reqs[e][v]*x[e,s,v] for e in endpoint_index for s in server_index for v in video_index)\n",
    "# the + y[s,v] it's used just to not let place useless video in cache server (but is not needed for this problem)\n",
    "obj = gp.quicksum((latency[e][s]*reqs[e][v]*x[e,s,v] + y[s,v])for e in endpoint_index for s in server_index for v in video_index)\n",
    "model.setObjective(obj, GRB.MINIMIZE)\n",
    "# model.setObjective(gp.quicksum(y[s,v] for s in server_index for v in video_index), GRB.MAXIMIZE)\n",
    "\n",
    "\n",
    "# constraints\n",
    "constr = (gp.quicksum(x[e,s,v] for e in endpoint_index)  <= num_endpoint*y[s,v] for s in server_index for v in video_index )\n",
    "model.addConstrs(constr, name=\"if video v available on server s\")\n",
    "\n",
    "constr = ( gp.quicksum( x[e,s,v] for s in server_index ) == (1 if reqs[e][v] else 0) for e in endpoint_index for v in video_index ) # datacenter excluded \n",
    "# constr = ( gp.quicksum( x[e,s,v] for s in latency[e] ) == (1 if reqs[e][v] else 0) for e in endpoint_index for v in video_index ) # latency[e] directly check existring connections\n",
    "model.addConstrs(constr, name=\"every request must be satisfied\")\n",
    "\n",
    "constr = ( gp.quicksum(video_size[v] * y[s,v] for v in video_index) <= cache_capacity for s in server_index[:-1] ) # -1 because datacenter have all the video\n",
    "model.addConstrs(constr, name=\"cache capacity\")\n",
    "\n",
    "\n",
    "constr = ( y[num_server,v] == 1 for v in video_index ) # cache servers are from 0 to s-1, s index (num_server) is for datacenter\n",
    "model.addConstrs(constr, name=\"Datacenter have all videos\")\n",
    "\n",
    "constr = ( gp.quicksum( x[e,s,v] for v in video_index ) <= (num_video*latency[e][s]) for e in endpoint_index for s in server_index[:-1] ) # -1 because datacenter have all the video\n",
    "model.addConstrs(constr, name=\"video v must be available on server s to be selected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61d392c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gurobi Optimizer version 12.0.1 build v12.0.1rc0 (linux64 - \"Arch Linux\")\n",
      "\n",
      "CPU model: AMD Ryzen 7 5700U with Radeon Graphics, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 8 physical cores, 16 logical processors, using up to 16 threads\n",
      "\n",
      "Optimize a model with 44 rows, 60 columns and 150 nonzeros\n",
      "Model fingerprint: 0xb013ed6b\n",
      "Variable types: 0 continuous, 60 integer (60 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+02]\n",
      "  Objective range  [2e+00, 2e+06]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 2e+03]\n",
      "Found heuristic solution: objective 2600012.0000\n",
      "Presolve removed 44 rows and 60 columns\n",
      "Presolve time: 0.01s\n",
      "Presolve: All rows and columns removed\n",
      "\n",
      "Explored 0 nodes (0 simplex iterations) in 0.02 seconds (0.00 work units)\n",
      "Thread count was 1 (of 16 available processors)\n",
      "\n",
      "Solution count 2: 1.25001e+06 2.60001e+06 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 1.250014000000e+06, best bound 1.250014000000e+06, gap 0.0000%\n"
     ]
    }
   ],
   "source": [
    "# Optimize the model\n",
    "model.optimize()\n",
    "\n",
    "# model.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16eebf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_full_output():\n",
    "    print(\"Optimal X [endpoint, server, video] values:\")\n",
    "    for e in endpoint_index:\n",
    "        for s in server_index:\n",
    "            for v in video_index:\n",
    "                print(f\"X[{e},{s},{v}] * {latency[e][s]} = {x[e,s,v]}\")\n",
    "    print(\"\\nOptimal Y [server, video] values:\")\n",
    "    for s in server_index:\n",
    "        for v in video_index:\n",
    "            print(f\"Y[{s},{v}] = {y[s,v]}\")\n",
    "\n",
    "def print_concise_output():\n",
    "    print(\"Optimal X [endpoint, server, video] values:\")\n",
    "    for e in endpoint_index:\n",
    "        for s in server_index:\n",
    "            for v in video_index:\n",
    "                if x[e,s,v].x:\n",
    "                    print(f\"X[{e},{s},{v}] * {latency[e][s]} = {x[e,s,v]}\")\n",
    "    print(\"\\nOptimal Y [server, video] values:\")\n",
    "    for s in server_index:\n",
    "        for v in video_index:\n",
    "            if y[s,v].x:\n",
    "                print(f\"Y[{s},{v}] = {y[s,v]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "54a6825c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimization successful!\n",
      "X[0,0,1] * 100 = <gurobi.Var x[0,0,1] (value 1.0)>\n",
      "X[0,0,3] * 100 = <gurobi.Var x[0,0,3] (value 1.0)>\n",
      "X[0,3,4] * 1000 = <gurobi.Var x[0,3,4] (value 1.0)>\n",
      "X[1,3,0] * 500 = <gurobi.Var x[1,3,0] (value 1.0)>\n",
      "\n",
      "Optimal Y values:\n",
      "Y[0,1] = <gurobi.Var y[0,1] (value 1.0)>\n",
      "Y[0,3] = <gurobi.Var y[0,3] (value 1.0)>\n",
      "Y[3,0] = <gurobi.Var y[3,0] (value 1.0)>\n",
      "Y[3,1] = <gurobi.Var y[3,1] (value 1.0)>\n",
      "Y[3,2] = <gurobi.Var y[3,2] (value 1.0)>\n",
      "Y[3,3] = <gurobi.Var y[3,3] (value 1.0)>\n",
      "Y[3,4] = <gurobi.Var y[3,4] (value 1.0)>\n",
      "\n",
      "Optimal objective value: 1250014.0\n"
     ]
    }
   ],
   "source": [
    "# results\n",
    "if model.status == gp.GRB.OPTIMAL:\n",
    "    print(\"\\nOptimization successful!\")\n",
    "    # print_full_output()\n",
    "    print_concise_output()\n",
    "    print(f\"\\nOptimal objective value: {model.objVal}\")\n",
    "    OPTIMAL_SOLUTION = model.ObjVal\n",
    "elif model.status == gp.GRB.INFEASIBLE:\n",
    "    print(\"Model is infeasible.\")\n",
    "elif model.status == gp.GRB.UNBOUNDED:\n",
    "    print(\"Model is unbounded.\")\n",
    "else:\n",
    "    print(f\"Optimization ended with status {model.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f13b29c",
   "metadata": {},
   "source": [
    "---\n",
    "# heuristics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd09de88",
   "metadata": {},
   "source": [
    "## Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d547ffa",
   "metadata": {},
   "source": [
    "### place video with highest request number in nearest cache when possible and place all videos for first endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039aa21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 3), (0, 1), (0, 4), (0, 0), (0, 2), (1, 0), (1, 1), (1, 2), (1, 3), (1, 4)]\n"
     ]
    }
   ],
   "source": [
    "# 1. sort by request number. use a list containing sorted indexes\n",
    "sorted_reqs = []\n",
    "E_IND = 0\n",
    "V_IND = 1\n",
    "\n",
    "for e in range(len(reqs)):\n",
    "    # Sort v indices by descending value for this e\n",
    "    sorted_vs = sorted(\n",
    "        range(len(reqs[e])),\n",
    "        key=lambda v: reqs[e][v],\n",
    "        reverse=True\n",
    "    )\n",
    "    # Add (e, v) pairs to the list\n",
    "    sorted_reqs.extend((e, v) for v in sorted_vs)\n",
    "\n",
    "\n",
    "# use a list to keep current cache capacity (will be decreased every time a video is placed in cache)\n",
    "curr_capacity = [cache_capacity for _ in range(num_server)]\n",
    "curr_capacity.append(float('inf')) # datacenter doesn't have capacity\n",
    "\n",
    "# create vars (simil guroby, used numpy for efficiency)\n",
    "x = np.zeros((num_endpoint, (num_server+1), num_video)) # e take v from s\n",
    "y = np.zeros((num_server+1), num_video) # v is in s\n",
    "\n",
    "#TODO: Order also latencies\n",
    "\n",
    "for req in sorted_reqs:\n",
    "    #TODO: Active x as 1 and decrease capacity by the order of this loop\n",
    "\n",
    "\n",
    "# print(reqs)\n",
    "# print(sorted_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d9e886",
   "metadata": {},
   "source": [
    "### place video with highest request number in nearest cache (every iteration change endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0775a67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87489dbd",
   "metadata": {},
   "source": [
    "### place video with highest request number in nearest cache when possible (order over both endpoint and video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e34ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. sort by request number. use a list containing sorted indexes\n",
    "sorted_indexes = sorted(\n",
    "    ((e, v) for e in range(len(reqs)) for v in range(len(reqs[e]))),\n",
    "    key=lambda pair: reqs[pair[0]][pair[1]],\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "# Split into two lists: one for e indices, one for v indices\n",
    "e_indexes = [e for e, v in sorted_indexes]\n",
    "v_indexes = [v for e, v in sorted_indexes]\n",
    "# print(reqs)\n",
    "# print(sorted_indexes)\n",
    "# print(\"e_indices:\", e_indexes)\n",
    "# print(\"v_indices:\", v_indexes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
