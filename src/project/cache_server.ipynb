{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9903b1c7",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea507c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b59b1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMAL_SOLUTION = 0 # used to evaluate gap between math model and heuristics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28210e55",
   "metadata": {},
   "source": [
    "---\n",
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6217fc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_video = 0\n",
    "num_endpoint = 0\n",
    "num_req_descriptions = 0\n",
    "num_server = 0\n",
    "\n",
    "cache_capacity = 0\n",
    "video_size = []\n",
    "\n",
    "latency = defaultdict(lambda: defaultdict(int))     # [endpoint][cache/datacenter] = latency\n",
    "reqs = defaultdict(lambda: defaultdict(int))        # [endpoint][video] = num reqs\n",
    "\n",
    "# dataset = \"dataset/videos_worth_spreading.in\"\n",
    "dataset = \"dataset/me_at_the_zoo.in\"\n",
    "# dataset = \"dataset/custom.in\"\n",
    "# dataset = \"dataset/minimal.in\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85803ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "status = 0\n",
    "curr_endpoint_index = 0\n",
    "num_connected_cache = 0\n",
    "with open(dataset, \"r\") as f:\n",
    "    for line_content in f:\n",
    "        line = line_content.split()\n",
    "\n",
    "        if status ==0:                                  # get counters\n",
    "            num_video = int(line[0])\n",
    "            num_endpoint = int(line[1])\n",
    "            num_req_descriptions = int(line[2])\n",
    "            num_server = int(line[3])\n",
    "            cache_capacity = int(line[4])\n",
    "            status = 1\n",
    "\n",
    "        elif status == 1:                               # get video dims\n",
    "            for size in line:\n",
    "                video_size.append(int(size))\n",
    "            status = 2\n",
    "\n",
    "        elif status == 2:                               # get datacenter latency and connected cache number\n",
    "            data_center_latency = int(line[0])\n",
    "            latency[curr_endpoint_index][num_server] = data_center_latency\n",
    "            \n",
    "            num_connected_cache = int(line[1])\n",
    "            if not num_connected_cache:\n",
    "                curr_endpoint_index = curr_endpoint_index + 1\n",
    "                if curr_endpoint_index == num_endpoint:\n",
    "                    status = 4\n",
    "            else:\n",
    "                status = 3\n",
    "        \n",
    "        elif status == 3:                                  # get cache latency\n",
    "            cache_index = int(line[0])\n",
    "            cache_latency = int(line[1])\n",
    "            latency[curr_endpoint_index][cache_index] = cache_latency\n",
    "            \n",
    "            num_connected_cache = num_connected_cache - 1\n",
    "            if not num_connected_cache:\n",
    "                curr_endpoint_index = curr_endpoint_index + 1\n",
    "                if curr_endpoint_index == num_endpoint:\n",
    "                    status = 4\n",
    "                else:\n",
    "                    status = 2\n",
    "        \n",
    "        elif status == 4:                                   # take num requests\n",
    "            video_index = int(line[0])\n",
    "            curr_endpoint_index = int(line[1])\n",
    "            num_reqs = int(line[2])\n",
    "            reqs[curr_endpoint_index][video_index] = num_reqs                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fb2d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common indexes\n",
    "endpoint_index = range(num_endpoint)\n",
    "server_index = range(num_server + 1) # I've modelled datacenter as last server\n",
    "video_index = range(num_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861cb172",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"num video: {num_video}, num endpoints {num_endpoint}, req descriptions {num_req_descriptions}, num cache {num_server}, dim {cache_capacity}\")\n",
    "print(f\"video sized: {video_size}\")\n",
    "print(f\"latencies: {latency}\")\n",
    "print(f\"reqs: {reqs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a922188b",
   "metadata": {},
   "source": [
    "---\n",
    "# Math model for Guroby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac327f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gp.Model(\"YoutubeCache\")\n",
    "\n",
    "# DECISION VARS\n",
    "x = model.addVars(endpoint_index, server_index, video_index, vtype=gp.GRB.BINARY, name=\"x\")\n",
    "y = model.addVars(server_index, video_index, vtype=gp.GRB.BINARY, name=\"y\")\n",
    "\n",
    "# OBJECTIVE FUNCTION\n",
    "obj = gp.quicksum(latency[e][s]*reqs[e][v]*x[e,s,v] for e in endpoint_index for s in server_index for v in video_index)\n",
    "# the + y[s,v] it's used just to not let place useless video in cache server (but is not needed for this problem)\n",
    "# obj = gp.quicksum((latency[e][s]*reqs[e][v]*x[e,s,v] + y[s,v])for e in endpoint_index for s in server_index for v in video_index)\n",
    "model.setObjective(obj, GRB.MINIMIZE)\n",
    "\n",
    "\n",
    "# CONSTRAINTS\n",
    "constr = (gp.quicksum(x[e,s,v] for e in endpoint_index)  <= num_endpoint*y[s,v] for s in server_index for v in video_index )\n",
    "model.addConstrs(constr, name=\"if video v available on server s\")\n",
    "\n",
    "constr = ( gp.quicksum( x[e,s,v] for s in server_index ) == (1 if reqs[e][v] else 0) for e in endpoint_index for v in video_index ) # datacenter excluded \n",
    "model.addConstrs(constr, name=\"every request must be satisfied\")\n",
    "\n",
    "constr = ( gp.quicksum(video_size[v] * y[s,v] for v in video_index) <= cache_capacity for s in server_index[:-1] ) # -1 because datacenter have all the video\n",
    "model.addConstrs(constr, name=\"cache capacity\")\n",
    "\n",
    "\n",
    "constr = ( y[num_server,v] == 1 for v in video_index ) # cache servers are from 0 to s-1, s index (num_server) is for datacenter\n",
    "model.addConstrs(constr, name=\"Datacenter have all videos\")\n",
    "\n",
    "constr = ( gp.quicksum( x[e,s,v] for v in video_index ) <= (num_video*latency[e][s]) for e in endpoint_index for s in server_index[:-1] ) # -1 because datacenter have all the video\n",
    "model.addConstrs(constr, name=\"video v must be available on server s to be selected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d392c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the model\n",
    "model.optimize()\n",
    "\n",
    "# model.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16eebf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model output\n",
    "\n",
    "def print_full_output():\n",
    "    print(\"Optimal X [endpoint, server, video] values:\")\n",
    "    for e in endpoint_index:\n",
    "        for s in server_index:\n",
    "            for v in video_index:\n",
    "                print(f\"X[{e},{s},{v}] * {latency[e][s]} = {x[e,s,v]}\")\n",
    "    print(\"\\nOptimal Y [server, video] values:\")\n",
    "    for s in server_index:\n",
    "        for v in video_index:\n",
    "            print(f\"Y[{s},{v}] = {y[s,v]}\")\n",
    "\n",
    "def print_concise_output():\n",
    "    print(\"Optimal X [endpoint, server, video] values:\")\n",
    "    for e in endpoint_index:\n",
    "        for s in server_index:\n",
    "            for v in video_index:\n",
    "                if x[e,s,v].x:\n",
    "                    print(f\"X[{e},{s},{v}] * {latency[e][s]} = {x[e,s,v]}\")\n",
    "    print(\"\\nOptimal Y [server, video] values:\")\n",
    "    for s in server_index:\n",
    "        for v in video_index:\n",
    "            if y[s,v].x:\n",
    "                print(f\"Y[{s},{v}] = {y[s,v]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a6825c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results\n",
    "if model.status == gp.GRB.OPTIMAL:\n",
    "    print(\"\\nOptimization successful!\")\n",
    "    # print_full_output()\n",
    "    print_concise_output()\n",
    "    print(f\"\\nOptimal objective value: {model.objVal}\")\n",
    "    OPTIMAL_SOLUTION = model.ObjVal\n",
    "elif model.status == gp.GRB.INFEASIBLE:\n",
    "    print(\"Model is infeasible.\")\n",
    "elif model.status == gp.GRB.UNBOUNDED:\n",
    "    print(\"Model is unbounded.\")\n",
    "else:\n",
    "    print(f\"Optimization ended with status {model.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f13b29c",
   "metadata": {},
   "source": [
    "---\n",
    "# Heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c355ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Common\n",
    "def compute_obj_func(x):\n",
    "    return sum(latency[e][s]*reqs[e][v]*x[e,s,v] for e in endpoint_index for s in server_index for v in video_index)\n",
    "\n",
    "x_sol = []\n",
    "y_sol = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd09de88",
   "metadata": {},
   "source": [
    "## Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d547ffa",
   "metadata": {},
   "source": [
    "### 1. place video with highest request number in nearest cache when possible and place all videos for the endpoint in the order that we get\n",
    "order video by request number, and for every endpoint retrieven from this ordered list place all its requested videos in best available caches for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039aa21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "E_IND = 0\n",
    "V_IND = 1\n",
    "\n",
    "# Sort v indexes by descending value for endpoint e\n",
    "sorted_reqs = []\n",
    "for e in range(len(reqs)):\n",
    "    sorted_vs = sorted(\n",
    "        [v for v in reqs[e] if reqs[e][v] != 0],\n",
    "        key=lambda v: reqs[e][v],\n",
    "        reverse=True\n",
    "    )\n",
    "    sorted_reqs.extend((e, v) for v in sorted_vs)\n",
    "\n",
    "# Sort server s latency for endpoint e\n",
    "sorted_latency = defaultdict(list)\n",
    "for e in latency:\n",
    "    sorted_s = sorted(\n",
    "        [s for s in latency[e] if latency[e][s] != 0],\n",
    "        key=lambda s: latency[e][s]\n",
    "    )\n",
    "    sorted_latency[e] = sorted_s\n",
    "\n",
    "\n",
    "# use a list to keep current cache capacity (will be decreased every time a video is placed in cache)\n",
    "curr_capacity = [cache_capacity for _ in range(num_server)]\n",
    "curr_capacity.append(float('inf')) # datacenter doesn't have capacity\n",
    "\n",
    "# create vars (simil guroby, used numpy for efficiency)\n",
    "x = np.zeros((num_endpoint, (num_server+1), num_video)) \n",
    "y = np.zeros(((num_server+1), num_video)) \n",
    "y[num_server, :] = 1 # datacenter keep all the videos\n",
    "\n",
    "for req in sorted_reqs:\n",
    "    req_endpoint = req[E_IND]\n",
    "    req_video = req[V_IND]\n",
    "    req_video_size = video_size[req_video]\n",
    "\n",
    "    for curr_cache_index in sorted_latency[req_endpoint]:\n",
    "        if y[curr_cache_index, req_video]:\n",
    "            x[req_endpoint, curr_cache_index, req_video] = 1\n",
    "            break\n",
    "        else:\n",
    "            if curr_capacity[curr_cache_index] > req_video_size:\n",
    "                curr_capacity[curr_cache_index] -= req_video_size\n",
    "                y[curr_cache_index, req_video] = 1\n",
    "                x[req_endpoint, curr_cache_index, req_video] = 1\n",
    "                break\n",
    "\n",
    "# print(\"X\")\n",
    "# print(x)\n",
    "# print(\"Y\")\n",
    "# print(y)\n",
    "APPROX_RESULT = compute_obj_func(x)\n",
    "GAP = ( abs(OPTIMAL_SOLUTION - APPROX_RESULT) / OPTIMAL_SOLUTION ) * 100\n",
    "print(f\"APPROX RESULT: {APPROX_RESULT} - GAP: {GAP}% - OPTIMAL RESULT {OPTIMAL_SOLUTION}\")\n",
    "x_sol.append((x, APPROX_RESULT))\n",
    "y_sol.append((y, APPROX_RESULT))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d9e886",
   "metadata": {},
   "source": [
    "### 2. place video with highest request number in nearest cache + round robin (every iteration change endpoint)\n",
    "order video by request number, use a round robin schedulo to choose an endpoint retrieven from this ordered list and place its remaining requested video in best available cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0775a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "E_IND = 0\n",
    "V_IND = 1\n",
    "\n",
    "# Sort v indexes by descending value for endpoint e\n",
    "sorted_reqs = defaultdict(list)\n",
    "for e in range(len(reqs)):\n",
    "    sorted_vs = sorted(\n",
    "        [v for v in reqs[e] if reqs[e][v] != 0],\n",
    "        key=lambda v: reqs[e][v],\n",
    "        reverse=True\n",
    "    )\n",
    "    sorted_reqs[e] = sorted_vs\n",
    "\n",
    "# Sort server s latency for endpoint e\n",
    "sorted_latency = defaultdict(list)\n",
    "for e in latency:\n",
    "    sorted_s = sorted(\n",
    "        [s for s in latency[e] if latency[e][s] != 0],\n",
    "        key=lambda s: latency[e][s]\n",
    "    )\n",
    "    sorted_latency[e] = sorted_s\n",
    "\n",
    "\n",
    "# use a list to keep current cache capacity (will be decreased every time a video is placed in cache)\n",
    "curr_capacity = [cache_capacity for _ in range(num_server)]\n",
    "curr_capacity.append(float('inf')) # datacenter doesn't have capacity\n",
    "\n",
    "# create vars (simil guroby, used numpy for efficiency)\n",
    "x = np.zeros((num_endpoint, (num_server+1), num_video))\n",
    "y = np.zeros(((num_server+1), num_video))\n",
    "y[num_server, :] = 1 # datacenter keep all the videos\n",
    "\n",
    "Done = False\n",
    "endpoints_req_index = [0 for _ in server_index]\n",
    "while not Done:\n",
    "    Done = True\n",
    "    for curr_endpoint in endpoint_index:\n",
    "        curr_endpoint_req_index = endpoints_req_index[curr_endpoint]\n",
    "        \n",
    "        if curr_endpoint_req_index < len(sorted_reqs[curr_endpoint]):\n",
    "            Done = False # There could still be reqs not satisfied other than this\n",
    "            req_video = sorted_reqs[curr_endpoint][curr_endpoint_req_index]\n",
    "            req_video_size = video_size[req_video]\n",
    "\n",
    "            for curr_cache_index in sorted_latency[curr_endpoint]:\n",
    "                if y[curr_cache_index, req_video]:\n",
    "                    x[curr_endpoint, curr_cache_index, req_video] = 1\n",
    "                    break\n",
    "                else:\n",
    "                    if curr_capacity[curr_cache_index] > req_video_size:\n",
    "                        curr_capacity[curr_cache_index] -= req_video_size\n",
    "                        y[curr_cache_index, req_video] = 1\n",
    "                        x[curr_endpoint, curr_cache_index, req_video] = 1\n",
    "                        break\n",
    "                    \n",
    "        endpoints_req_index[curr_endpoint] += 1\n",
    "\n",
    "# print(\"X\")\n",
    "# print(x)\n",
    "# print(\"Y\")\n",
    "# print(y)\n",
    "APPROX_RESULT = compute_obj_func(x)\n",
    "GAP = ( abs(OPTIMAL_SOLUTION - APPROX_RESULT) / OPTIMAL_SOLUTION ) * 100\n",
    "print(f\"APPROX RESULT: {APPROX_RESULT} - GAP: {GAP}% - OPTIMAL RESULT {OPTIMAL_SOLUTION}\")\n",
    "x_sol.append((x, APPROX_RESULT))\n",
    "y_sol.append((y, APPROX_RESULT))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87489dbd",
   "metadata": {},
   "source": [
    "### 3. place video with highest request number in nearest cache when possible\n",
    "(only order by request number without considering the endpoints, pratically place cache video in the best server order by request number withouth reasoning on endpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e34ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "E_IND = 0\n",
    "V_IND = 1\n",
    "\n",
    "# Sort v indexes by descending value for endpoint e\n",
    "sorted_reqs = []\n",
    "sorted_reqs = sorted(\n",
    "    [(e, v) for e in range(len(reqs)) for v in reqs[e] if reqs[e][v] != 0],\n",
    "    key=lambda pair: reqs[pair[0]][pair[1]],\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "# Sort server s latency for endpoint e\n",
    "sorted_latency = defaultdict(list)\n",
    "for e in latency:\n",
    "    sorted_s = sorted(\n",
    "        [s for s in latency[e] if latency[e][s] != 0],\n",
    "        key=lambda s: latency[e][s]\n",
    "    )\n",
    "    sorted_latency[e] = sorted_s\n",
    "\n",
    "\n",
    "# use a list to keep current cache capacity (will be decreased every time a video is placed in cache)\n",
    "curr_capacity = [cache_capacity for _ in range(num_server)]\n",
    "curr_capacity.append(float('inf')) # datacenter doesn't have capacity\n",
    "\n",
    "# create vars (simil guroby, used numpy for efficiency)\n",
    "x = np.zeros((num_endpoint, (num_server+1), num_video))\n",
    "y = np.zeros(((num_server+1), num_video))\n",
    "y[num_server, :] = 1 # datacenter keep all the videos\n",
    "\n",
    "for req_endpoint,req_video in sorted_reqs:\n",
    "    req_video_size = video_size[req_video]\n",
    "\n",
    "    for curr_cache_index in sorted_latency[req_endpoint]:\n",
    "        if y[curr_cache_index, req_video]:\n",
    "            x[req_endpoint, curr_cache_index, req_video] = 1\n",
    "            break\n",
    "        else:\n",
    "            if curr_capacity[curr_cache_index] > req_video_size:\n",
    "                curr_capacity[curr_cache_index] -= req_video_size\n",
    "                y[curr_cache_index, req_video] = 1\n",
    "                x[req_endpoint, curr_cache_index, req_video] = 1\n",
    "                break\n",
    "\n",
    "# print(\"X\")\n",
    "# print(x)\n",
    "# print(\"Y\")\n",
    "# print(y)\n",
    "APPROX_RESULT = compute_obj_func(x)\n",
    "GAP = ( abs(OPTIMAL_SOLUTION - APPROX_RESULT) / OPTIMAL_SOLUTION ) * 100\n",
    "print(f\"APPROX RESULT: {APPROX_RESULT} - GAP: {GAP}% - OPTIMAL RESULT {OPTIMAL_SOLUTION}\")\n",
    "x_sol.append((x, APPROX_RESULT))\n",
    "y_sol.append((y, APPROX_RESULT))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0dd1d5",
   "metadata": {},
   "source": [
    "### 4. Place video ordered by popularity in cache with most connected endpoints for that video\n",
    "Order video by request number and place ordered video in the cache connected with most endpoints that have requested that specific video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa52075",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_req_count = [0 for _ in video_index]\n",
    "latency_sum = defaultdict(lambda: defaultdict(lambda: defaultdict(int))) # used to find best cache to place a video [server][video][latency sum / num endpoint / score]\n",
    "LATENCY_INDEX = 0\n",
    "NUM_ENDPOINT_CONNECTED_INDEX = 1\n",
    "SCORE_INDEX = 2\n",
    "\n",
    "for curr_endpoint_index, endpoint_reqs in reqs.items():\n",
    "    for curr_video_index, req_num in endpoint_reqs.items():\n",
    "        if req_num: # check if requests from endpoint for the video exists\n",
    "            video_req_count[curr_video_index] += req_num\n",
    "            for curr_server_index, lat in latency[curr_endpoint_index].items():\n",
    "                if curr_server_index != num_server and lat:\n",
    "                    latency_sum[curr_server_index][curr_video_index][LATENCY_INDEX] += lat\n",
    "                    latency_sum[curr_server_index][curr_video_index][NUM_ENDPOINT_CONNECTED_INDEX] += 1\n",
    "                    latency_sum[curr_server_index][curr_video_index][SCORE_INDEX] = (\n",
    "                        latency_sum[curr_server_index][curr_video_index][NUM_ENDPOINT_CONNECTED_INDEX] \n",
    "                        /\n",
    "                        latency_sum[curr_server_index][curr_video_index][LATENCY_INDEX]\n",
    "                    )\n",
    "            \n",
    "sorted_video_indexes = sorted(range(num_video), key=lambda i: video_req_count[i], reverse=True)\n",
    "\n",
    "# use a list to keep current cache capacity (will be decreased every time a video is placed in cache)\n",
    "curr_capacity = [cache_capacity for _ in range(num_server)]\n",
    "curr_capacity.append(float('inf')) # datacenter doesn't have capacity\n",
    "# print(curr_capacity)\n",
    "\n",
    "# create vars (simil guroby, used numpy for efficiency)\n",
    "x = np.zeros((num_endpoint, (num_server+1), num_video)) # e take v from s\n",
    "y = np.zeros(((num_server+1), num_video)) # v is in s\n",
    "y[num_server, :] = 1 # datacenter keep all the videos\n",
    "\n",
    "# Sort server s latency for endpoint e\n",
    "sorted_latency = defaultdict(list)\n",
    "for e in latency:\n",
    "    # Sort v indices by ascending value for this e\n",
    "    sorted_s = sorted(\n",
    "        [s for s in latency[e] if latency[e][s] != 0],\n",
    "        key=lambda s: latency[e][s]\n",
    "    )\n",
    "    sorted_latency[e] = sorted_s\n",
    "\n",
    "\n",
    "for curr_video_index in sorted_video_indexes:\n",
    "    # now sort caches to get the ones with most connected endpoints that have requested video curr_video_index\n",
    "    for curr_server_index in sorted(\n",
    "        [\n",
    "            i\n",
    "            for i in range(len(latency_sum))\n",
    "            if latency_sum[i][curr_video_index][NUM_ENDPOINT_CONNECTED_INDEX] != 0\n",
    "        ],\n",
    "        key=lambda i: latency_sum[i][curr_video_index][NUM_ENDPOINT_CONNECTED_INDEX],\n",
    "        reverse=True\n",
    "    ):\n",
    "        if not y[curr_server_index, curr_video_index]:\n",
    "            if curr_capacity[curr_server_index] > video_size[curr_video_index]:\n",
    "                curr_capacity[curr_server_index] -= video_size[curr_video_index]\n",
    "                y[curr_server_index, curr_video_index] = 1\n",
    "                break\n",
    "\n",
    "# iterate trough all reqs to check to what server the endpoint should request the video\n",
    "for req_endpoint,req_videos in reqs.items():    \n",
    "    for curr_video_index in req_videos:\n",
    "        if reqs[req_endpoint][curr_video_index]:\n",
    "            for curr_server_index in sorted_latency[req_endpoint]:\n",
    "                if y[curr_server_index, curr_video_index]:\n",
    "                    x[req_endpoint, curr_server_index, curr_video_index] = 1\n",
    "                    break\n",
    "\n",
    "# print(\"X\")\n",
    "# print(x)\n",
    "# print(\"Y\")\n",
    "# print(y)\n",
    "APPROX_RESULT = compute_obj_func(x)\n",
    "GAP = ( abs(OPTIMAL_SOLUTION - APPROX_RESULT) / OPTIMAL_SOLUTION ) * 100\n",
    "print(f\"APPROX RESULT: {APPROX_RESULT} - GAP: {GAP}% - OPTIMAL RESULT {OPTIMAL_SOLUTION}\")\n",
    "x_sol.append((x, APPROX_RESULT))\n",
    "y_sol.append((y, APPROX_RESULT))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f66908d",
   "metadata": {},
   "source": [
    "### 5. place video based on connected endpoint most requested videos\n",
    "for every caches, place their most requested video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702ccb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "E_IND = 0\n",
    "V_IND = 1\n",
    "\n",
    "# Sort v indexes by descending value for endpoint e\n",
    "sorted_reqs = []\n",
    "sorted_reqs = sorted(\n",
    "    [(e, v) for e in range(len(reqs)) for v in reqs[e] if reqs[e][v] != 0],\n",
    "    key=lambda pair: reqs[pair[0]][pair[1]],\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "# Sort server s latency for endpoint e\n",
    "sorted_latency = defaultdict(list)\n",
    "for e in latency:\n",
    "    sorted_s = sorted(\n",
    "        [s for s in latency[e] if latency[e][s] != 0],\n",
    "        key=lambda s: latency[e][s]\n",
    "    )\n",
    "    sorted_latency[e] = sorted_s\n",
    "\n",
    "\n",
    "# use a list to keep current cache capacity (will be decreased every time a video is placed in cache)\n",
    "curr_capacity = [cache_capacity for _ in range(num_server)]\n",
    "curr_capacity.append(float('inf')) # datacenter doesn't have capacity\n",
    "\n",
    "# create vars (simil guroby, used numpy for efficiency)\n",
    "x = np.zeros((num_endpoint, (num_server+1), num_video)) # e take v from s\n",
    "y = np.zeros(((num_server+1), num_video)) # v is in s\n",
    "y[num_server, :] = 1 # datacenter keep all the videos\n",
    "\n",
    "\n",
    "# get total possible reqs for any cache server from its connected endpoints\n",
    "server_total_reqs = defaultdict(lambda: defaultdict(int))\n",
    "for req_endpoint,req_videos in reqs.items():\n",
    "    for curr_server_index, lat in latency[req_endpoint].items():\n",
    "                if curr_server_index != num_server and lat:\n",
    "                    for curr_video_index in req_videos.keys():\n",
    "                        server_total_reqs[curr_server_index][curr_video_index] += req_videos[curr_video_index]\n",
    "\n",
    "\n",
    "# place video in caches based on request counts\n",
    "for curr_server_index in server_index[:-1]:\n",
    "    sorted_indexes_only = [\n",
    "        video_index\n",
    "        for video_index, count in sorted(\n",
    "            server_total_reqs[curr_server_index].items(),\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )\n",
    "        if count > 0\n",
    "    ]\n",
    "    for curr_video_index in sorted_indexes_only:\n",
    "        if not y[curr_server_index, curr_video_index] and curr_capacity[curr_server_index] > video_size[curr_video_index]:\n",
    "            curr_capacity[curr_server_index] -= video_size[curr_video_index]\n",
    "            y[curr_server_index, curr_video_index] = 1\n",
    "\n",
    "\n",
    "# iterate trough all reqs to check to what server the endpoint should request the video\n",
    "for req_endpoint,req_videos in reqs.items():    \n",
    "    for curr_video_index in req_videos:\n",
    "        if reqs[req_endpoint][curr_video_index]:\n",
    "            for curr_server_index in sorted_latency[req_endpoint]:\n",
    "                if y[curr_server_index, curr_video_index]:\n",
    "                    x[req_endpoint, curr_server_index, curr_video_index] = 1\n",
    "                    break\n",
    "\n",
    "# print(\"X\")\n",
    "# print(x)\n",
    "# print(\"Y\")\n",
    "# print(y)\n",
    "APPROX_RESULT = compute_obj_func(x)\n",
    "GAP = ( abs(OPTIMAL_SOLUTION - APPROX_RESULT) / OPTIMAL_SOLUTION ) * 100\n",
    "print(f\"APPROX RESULT: {APPROX_RESULT} - GAP: {GAP}% - OPTIMAL RESULT {OPTIMAL_SOLUTION}\")\n",
    "x_sol.append((x, APPROX_RESULT))\n",
    "y_sol.append((y, APPROX_RESULT))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e63615d",
   "metadata": {},
   "source": [
    "## Local Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab0c175",
   "metadata": {},
   "source": [
    "### tabu search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ff2d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_x(y):\n",
    "    x = np.zeros((num_endpoint, (num_server+1), num_video))\n",
    "\n",
    "    # Sort server s latency for endpoint e\n",
    "    sorted_latency = defaultdict(list)\n",
    "    for e in latency:\n",
    "        sorted_s = sorted(\n",
    "            [s for s in latency[e] if latency[e][s] != 0],\n",
    "            key=lambda s: latency[e][s]\n",
    "        )\n",
    "        sorted_latency[e] = sorted_s\n",
    "    \n",
    "    for curr_endpoint_index in endpoint_index:\n",
    "        for curr_video_index in reqs[curr_endpoint_index]:\n",
    "            best_server_index = num_server  # default to datacenter\n",
    "            \n",
    "            for curr_server_index in sorted_latency[curr_endpoint_index]:                    \n",
    "                if y[curr_server_index][curr_video_index]:\n",
    "                    best_server_index = curr_server_index\n",
    "                    break\n",
    "\n",
    "            x[curr_endpoint_index][best_server_index][curr_video_index] = 1\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5369a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabu_search_toggle(x, y, num_iters=1000, tabu_list_dim=10):\n",
    "    tabu_list = deque(maxlen=tabu_list_dim)\n",
    "    intensification_list = []\n",
    "    \n",
    "    # Start with initial solution\n",
    "    best_x = np.copy(x)\n",
    "    best_y = np.copy(y)\n",
    "    best_obj_val = compute_obj_func(best_x)\n",
    "\n",
    "    for iteration in range(num_iters):\n",
    "        neighborhood = []\n",
    "\n",
    "        # Generate neighbor solutions\n",
    "        for curr_server_index in server_index[:-1]:  # Only cache servers\n",
    "            for curr_video_index in video_index:\n",
    "                move = (curr_server_index, curr_video_index)\n",
    "                if move in tabu_list:\n",
    "                    continue\n",
    "\n",
    "                # Try toggling video v in cache s\n",
    "                new_y = np.copy(y)\n",
    "                new_y[curr_server_index][curr_video_index] = 1 - new_y[curr_server_index][curr_video_index]\n",
    "\n",
    "                # Check cache capacity constraint\n",
    "                curr_video_size = sum(video_size[v] for v in video_index if new_y[curr_server_index][v])\n",
    "                if curr_video_size > cache_capacity:\n",
    "                    continue\n",
    "\n",
    "                # Generate new x according to new y\n",
    "                new_x = get_best_x(new_y)\n",
    "\n",
    "                obj_val = compute_obj_func(new_x)\n",
    "\n",
    "                neighborhood.append((obj_val, new_x, new_y, move))\n",
    "\n",
    "        if not neighborhood:\n",
    "            break\n",
    "\n",
    "        # Choose best neighbor\n",
    "        neighborhood.sort(key=lambda tup: tup[0])  # Sort by delay\n",
    "        obj_val, new_x, new_y, move = neighborhood[0]\n",
    "\n",
    "        if obj_val < best_obj_val:\n",
    "            best_obj_val = obj_val\n",
    "            best_x = new_x\n",
    "            best_y = new_y\n",
    "            intensification_list.append((best_obj_val, best_x, best_y))\n",
    "            intensification_list = sorted(intensification_list)[:10]  # keep top 10\n",
    "\n",
    "        # Update current solution\n",
    "        x = new_x\n",
    "        y = new_y\n",
    "\n",
    "        # Update tabu list\n",
    "        tabu_list.append(move)\n",
    "        \n",
    "        # Random intensification\n",
    "        if iteration % 70 == 0 and intensification_list:\n",
    "            _, best_x, best_y = random.choice(intensification_list)\n",
    "            x, y = best_x.copy(), best_y.copy()\n",
    "\n",
    "    return best_x, best_y, best_obj_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136e156b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabu_search_add(x, y, num_iters=1000, tabu_list_dim=10):\n",
    "    tabu_list = deque(maxlen=tabu_list_dim)\n",
    "    intensification_list = []\n",
    "    \n",
    "    # Start with initial solution\n",
    "    best_x = np.copy(x)\n",
    "    best_y = np.copy(y)\n",
    "    best_obj_val = compute_obj_func(best_x)\n",
    "\n",
    "    for iteration in range(num_iters):\n",
    "        neighborhood = []\n",
    "\n",
    "        # Generate neighbor solutions\n",
    "        for curr_server_index in server_index[:-1]:  # Only cache servers\n",
    "            for curr_video_index in video_index:\n",
    "                move = (curr_server_index, curr_video_index)\n",
    "                if move in tabu_list:\n",
    "                    continue\n",
    "\n",
    "                # Try toggling video v in cache s\n",
    "                new_y = np.copy(y)\n",
    "                \n",
    "                # Try adding current video in current server if not already present\n",
    "                if new_y[curr_server_index][curr_video_index]:\n",
    "                    continue\n",
    "                new_y[curr_server_index][curr_video_index] = 1 \n",
    "\n",
    "                # Check cache capacity constraint\n",
    "                curr_video_size = sum(video_size[v] for v in video_index if new_y[curr_server_index][v])\n",
    "                if curr_video_size > cache_capacity:\n",
    "                    continue\n",
    "\n",
    "                # Generate new x according to new y\n",
    "                new_x = get_best_x(new_y)\n",
    "\n",
    "                obj_val = compute_obj_func(new_x)\n",
    "\n",
    "                neighborhood.append((obj_val, new_x, new_y, move))\n",
    "\n",
    "        if not neighborhood:\n",
    "            break\n",
    "\n",
    "        # Choose best neighbor\n",
    "        neighborhood.sort(key=lambda tup: tup[0])  # Sort by delay\n",
    "        obj_val, new_x, new_y, move = neighborhood[0]\n",
    "\n",
    "        if obj_val < best_obj_val:\n",
    "            best_obj_val = obj_val\n",
    "            best_x = new_x\n",
    "            best_y = new_y\n",
    "            intensification_list.append((best_obj_val, best_x, best_y))\n",
    "            intensification_list = sorted(intensification_list)[:10]  # keep top 10\n",
    "\n",
    "        # Update current solution\n",
    "        x = new_x\n",
    "        y = new_y\n",
    "\n",
    "        # Update tabu list\n",
    "        tabu_list.append(move)\n",
    "        \n",
    "        # Random intensification\n",
    "        if iteration % 70 == 0 and intensification_list:\n",
    "            _, best_x, best_y = random.choice(intensification_list)\n",
    "            x, y = best_x.copy(), best_y.copy()\n",
    "\n",
    "    return best_x, best_y, best_obj_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24d4971",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"best sol: {OPTIMAL_SOLUTION}\")\n",
    "tabu_list_dim = np.floor(np.sqrt(num_req_descriptions))\n",
    "\n",
    "for sol_index in range(len(x_sol)):\n",
    "    x_tabu_toggle, y_tabu_toggle, obj_tabu_toggle = tabu_search_toggle(x_sol[sol_index][0], y_sol[sol_index][0], tabu_list_dim=tabu_list_dim)\n",
    "    print(f\"old solution: {compute_obj_func(x_sol[sol_index][0])}, tabu sol: {obj_tabu_toggle}\")\n",
    "    \n",
    "    x_tabu_add, y_tabu_add, obj_tabu_add = tabu_search_add(x_sol[sol_index][0], y_sol[sol_index][0], tabu_list_dim=tabu_list_dim)\n",
    "    print(f\"old solution: {compute_obj_func(x_sol[sol_index][0])}, tabu sol: {obj_tabu_add}\")\n",
    "    \n",
    "    if obj_tabu_toggle > obj_tabu_add:\n",
    "        x_sol.append((x_tabu_add, obj_tabu_add))\n",
    "        y_sol.append((y_tabu_add, obj_tabu_add))\n",
    "    else:\n",
    "        x_sol.append((x_tabu_toggle, obj_tabu_toggle))\n",
    "        y_sol.append((y_tabu_toggle, obj_tabu_toggle))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7701ef4",
   "metadata": {},
   "source": [
    "## Genetic algorithm\n",
    "\n",
    "#### fitness: minimize delay ( compute_obj_func(x) )\n",
    "#### initial population: heuristics solutions (x_sol & y_sol)\n",
    "#### randomization: montecarlo simulation\n",
    "#### crossover & mutation: on y (update x conseguently get_best_x(y) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0566772",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_IND = 0\n",
    "Y_IND = 1\n",
    "OBJ_IND = 2\n",
    "\n",
    "POP_SIZE = 50\n",
    "NGEN = 100\n",
    "MUT_RATE = 0.001\n",
    "\n",
    "def is_feasible(y):\n",
    "    for curr_server_index in server_index:\n",
    "        # calculate current size when video is placed on specific cache\n",
    "        curr_tot_size = sum(video_size[curr_video_index] for curr_video_index in video_index if y[curr_server_index, curr_video_index])\n",
    "        if curr_tot_size < cache_capacity[curr_server_index]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def montecarlo_roulette_individual_selection(current_population, reverse=False):\n",
    "    if reverse:\n",
    "        fitness_list = [ curr_individual[OBJ_IND] for curr_individual in current_population ]\n",
    "    else:\n",
    "        # 1 / obj func because less delay will have bigger size\n",
    "        fitness_list = [ (1/curr_individual[OBJ_IND]) for curr_individual in current_population ]\n",
    "        \n",
    "    fitness_tot = sum(fitness_list) - 1 #-1 for randint generation\n",
    "    \n",
    "    spin = random.randint(0, fitness_tot)\n",
    "    \n",
    "    curr_fit = 0\n",
    "    for curr_individual_index in range(len(current_population)):\n",
    "        curr_fit += fitness_list[curr_individual_index]\n",
    "        if spin < curr_fit:\n",
    "            return current_population[curr_individual_index]\n",
    "\n",
    "def crossover(parent1, parent2):\n",
    "    # we need to clone parent or python will reference to them (and if we do mutation we'll do it also on parents)\n",
    "    y1 = parent1[1].copy()\n",
    "    y2 = parent2[1].copy()\n",
    "    \n",
    "    childs_y = []\n",
    "    childs = []\n",
    "    \n",
    "    # Monosplit crossover\n",
    "    split = np.random.randint(1, num_video)\n",
    "    # take y rows [0 to split] from parent 1 and y rows [split to num_server+1] from parent2 \n",
    "    childs_y.append( np.vstack([y1[:,:split], y2[:,split:]]) )\n",
    "    \n",
    "    # kill unfeasible child (spartan way)\n",
    "    for child_y in childs_y:\n",
    "        if not is_feasible(child_y):\n",
    "            continue\n",
    "        \n",
    "        child_x = get_best_x(child_y)\n",
    "        childs.append(child_x, child_y, compute_obj_func(child_x))\n",
    "    \n",
    "    return childs\n",
    "\n",
    "#TODO RECHECK THIS METHOD AND IMPROVE IT\n",
    "def mutate(childs):\n",
    "\n",
    "    childs_mutated = childs.copy()\n",
    "    \n",
    "    for child in childs:\n",
    "        child_unmutated = child.copy()\n",
    "        for curr_server_index in server_index[:-1]:\n",
    "            for curr_video_index in video_index:\n",
    "                if random.random() < MUT_RATE:\n",
    "                    child[Y_IND][curr_server_index][curr_video_index] ^= 1 # toggle mutation\n",
    "                    \n",
    "        if is_feasible(y):\n",
    "            childs_mutated.remove(child_unmutated)\n",
    "            childs_mutated.append(child)\n",
    "            \n",
    "    return childs_mutated\n",
    "\n",
    "# Main GA loop\n",
    "def run_ga():\n",
    "    # an individual is (x, y, obj value)\n",
    "    population = [(x_sol[curr_individual_index][0], y_sol[curr_individual_index][0], x_sol[curr_individual_index][1]) for curr_individual_index in range(len(y_sol))]\n",
    "    \n",
    "    for gen in range(NGEN):\n",
    "        for _ in range(3): # generate 6 individual every iteration\n",
    "            parent_1 = montecarlo_roulette_individual_selection(population)\n",
    "            parent_2 = montecarlo_roulette_individual_selection(population)\n",
    "            childs = crossover(parent_1, parent_2)\n",
    "            childs = mutate(childs)\n",
    "            population.append(childs)\n",
    "        \n",
    "        for _ in range(2): # remove 2 individual every iteration\n",
    "            dead = montecarlo_roulette_individual_selection(population, reverse=True)\n",
    "            population.remove(dead)\n",
    "        \n",
    "        best = min(population, key=lambda ind: compute_obj_func(ind[0]))\n",
    "        print(f\"Gen {gen}: Best delay = {compute_obj_func(best[0])}\")\n",
    "    return min(population, key=lambda ind: compute_obj_func(ind[0]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
