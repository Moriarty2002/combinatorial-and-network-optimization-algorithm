{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9903b1c7",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "3ea507c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28210e55",
   "metadata": {},
   "source": [
    "---\n",
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "6217fc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_video = 0\n",
    "num_endpoint = 0\n",
    "num_req_descriptions = 0\n",
    "num_server = 0\n",
    "\n",
    "cache_capacity = 0\n",
    "video_size = []\n",
    "\n",
    "latency = defaultdict(lambda: defaultdict(int))                         # [endpoint][cache/datacenter] = latency\n",
    "reqs = defaultdict(lambda: defaultdict(int))        # [endpoint][video] = num reqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "c85803ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = \"dataset/videos_worth_spreading.in\"\n",
    "# dataset = \"dataset/me_at_the_zoo.in\"\n",
    "# dataset = \"dataset/custom.in\"\n",
    "dataset = \"dataset/minimal.in\"\n",
    "\n",
    "status = 0\n",
    "endpoint_index = 0\n",
    "num_connected_cache = 0\n",
    "with open(dataset, \"r\") as f:\n",
    "    for line_content in f:\n",
    "        line = line_content.split()\n",
    "\n",
    "        if status ==0:                                  # get counters\n",
    "            num_video = int(line[0])\n",
    "            num_endpoint = int(line[1])\n",
    "            num_req_descriptions = int(line[2])\n",
    "            num_server = int(line[3])\n",
    "            cache_capacity = int(line[4])\n",
    "            status = 1\n",
    "\n",
    "        elif status == 1:                               # get video dims\n",
    "            for size in line:\n",
    "                video_size.append(int(size))\n",
    "            status = 2\n",
    "\n",
    "        elif status == 2:                               # get datacenter latency and connected cache number\n",
    "            data_center_latency = int(line[0])\n",
    "            latency[endpoint_index][num_server] = data_center_latency\n",
    "            \n",
    "            num_connected_cache = int(line[1])\n",
    "            if not num_connected_cache:\n",
    "                endpoint_index = endpoint_index + 1\n",
    "                if endpoint_index == num_endpoint:\n",
    "                    status = 4\n",
    "            else:\n",
    "                status = 3\n",
    "        \n",
    "        elif status == 3:                                  # get cache latency\n",
    "            cache_index = int(line[0])\n",
    "            cache_latency = int(line[1])\n",
    "            latency[endpoint_index][cache_index] = cache_latency\n",
    "            \n",
    "            num_connected_cache = num_connected_cache - 1\n",
    "            if not num_connected_cache:\n",
    "                endpoint_index = endpoint_index + 1\n",
    "                if endpoint_index == num_endpoint:\n",
    "                    status = 4\n",
    "                else:\n",
    "                    status = 2\n",
    "        \n",
    "        elif status == 4:                                   # take num requests\n",
    "            video_index = int(line[0])\n",
    "            endpoint_index = int(line[1])\n",
    "            num_reqs = int(line[2])\n",
    "            reqs[endpoint_index][video_index] = num_reqs                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "861cb172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num video: 2, num endpoints 1, req descriptions 2, num cache 1, dim 100\n",
      "video sized: [50, 50]\n",
      "latencies: defaultdict(<function <lambda> at 0x7f978c785b20>, {0: defaultdict(<class 'int'>, {1: 1000, 0: 100})})\n",
      "reqs: defaultdict(<function <lambda> at 0x7f9795de9760>, {0: defaultdict(<class 'int'>, {0: 1000, 1: 500})})\n"
     ]
    }
   ],
   "source": [
    "print(f\"num video: {num_video}, num endpoints {num_endpoint}, req descriptions {num_req_descriptions}, num cache {num_server}, dim {cache_capacity}\")\n",
    "print(f\"video sized: {video_size}\")\n",
    "print(f\"latencies: {latency}\")\n",
    "print(f\"reqs: {reqs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a922188b",
   "metadata": {},
   "source": [
    "---\n",
    "# Math model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "47768109",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_index = range(num_endpoint)\n",
    "server_index = range(num_server + 1) # I've modelled datacenter as last server\n",
    "video_index = range(num_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "bac327f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): <gurobi.Constr *Awaiting Model Update*>}"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gp.Model(\"YoutubeCache\")\n",
    "\n",
    "# decision vars\n",
    "x = model.addVars(endpoint_index, server_index, video_index, vtype=gp.GRB.BINARY, name=\"x\")\n",
    "y = model.addVars(server_index, video_index, vtype=gp.GRB.BINARY, name=\"y\")\n",
    "\n",
    "# objective function\n",
    "obj = gp.quicksum(latency[e][s]*reqs[e][v]*x[e,s,v] for e in endpoint_index for s in server_index for v in video_index)\n",
    "model.setObjective(obj, GRB.MINIMIZE)\n",
    "# model.setObjective(gp.quicksum(y[s,v] for s in server_index for v in video_index), GRB.MAXIMIZE)\n",
    "\n",
    "\n",
    "# constraints\n",
    "constr = (gp.quicksum(x[e,s,v] for e in endpoint_index)  <= num_endpoint*y[s,v] for s in server_index for v in video_index )\n",
    "model.addConstrs(constr, name=\"if video v available on server s\")\n",
    "\n",
    "constr = ( gp.quicksum( x[e,s,v] for s in server_index ) == (1 if reqs[e][v] else 0) for e in endpoint_index for v in video_index ) # datacenter excluded \n",
    "# constr = ( gp.quicksum( x[e,s,v] for s in latency[e] ) == (1 if reqs[e][v] else 0) for e in endpoint_index for v in video_index ) # latency[e] directly check existring connections\n",
    "model.addConstrs(constr, name=\"every request must be satisfied\")\n",
    "\n",
    "constr = ( gp.quicksum(video_size[v] * y[s,v] for v in video_index) <= cache_capacity for s in server_index[:-1] ) # -1 because datacenter have all the video\n",
    "model.addConstrs(constr, name=\"cache capacity\")\n",
    "\n",
    "\n",
    "constr = ( y[num_server,v] == 1 for v in video_index ) # cache servers are from 0 to s-1, s index (num_server) is for datacenter\n",
    "model.addConstrs(constr, name=\"Datacenter have all videos\")\n",
    "\n",
    "constr = ( gp.quicksum( x[e,s,v] for v in video_index ) <= (num_video*latency[e][s]) for e in endpoint_index for s in server_index[:-1] ) # -1 because datacenter have all the video\n",
    "model.addConstrs(constr, name=\"video v must be available on server s to be selected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "61d392c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gurobi Optimizer version 12.0.1 build v12.0.1rc0 (linux64 - \"Arch Linux\")\n",
      "\n",
      "CPU model: AMD Ryzen 7 5700U with Radeon Graphics, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 8 physical cores, 16 logical processors, using up to 16 threads\n",
      "\n",
      "Optimize a model with 10 rows, 8 columns and 18 nonzeros\n",
      "Model fingerprint: 0xc33683a7\n",
      "Variable types: 0 continuous, 8 integer (8 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 5e+01]\n",
      "  Objective range  [5e+04, 1e+06]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 2e+02]\n",
      "Found heuristic solution: objective 1500000.0000\n",
      "Presolve removed 10 rows and 8 columns\n",
      "Presolve time: 0.00s\n",
      "Presolve: All rows and columns removed\n",
      "\n",
      "Explored 0 nodes (0 simplex iterations) in 0.01 seconds (0.00 work units)\n",
      "Thread count was 1 (of 16 available processors)\n",
      "\n",
      "Solution count 2: 150000 1.5e+06 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 1.500000000000e+05, best bound 1.500000000000e+05, gap 0.0000%\n",
      "Minimize\n",
      "  100000.0 x[0,0,0] + 50000.0 x[0,0,1] + 1000000.0 x[0,1,0] + 500000.0 x[0,1,1]\n",
      "Subject To\n",
      "  if video v available on server s[0,0]: x[0,0,0] + -1.0 y[0,0] <= 0\n",
      "  if video v available on server s[0,1]: x[0,0,1] + -1.0 y[0,1] <= 0\n",
      "  if video v available on server s[1,0]: x[0,1,0] + -1.0 y[1,0] <= 0\n",
      "  if video v available on server s[1,1]: x[0,1,1] + -1.0 y[1,1] <= 0\n",
      "  every request must be satisfied[0,0]: x[0,0,0] + x[0,1,0] = 1\n",
      "  every request must be satisfied[0,1]: x[0,0,1] + x[0,1,1] = 1\n",
      "  cache capacity[0]: 50.0 y[0,0] + 50.0 y[0,1] <= 100\n",
      "  Datacenter have all videos[0]: y[1,0] = 1\n",
      "  Datacenter have all videos[1]: y[1,1] = 1\n",
      "  video v must be available on server s to be selected[0,0]: x[0,0,0] + x[0,0,1] <= 200\n",
      "Binaries\n",
      "  ['x[0,0,0]', 'x[0,0,1]', 'x[0,1,0]', 'x[0,1,1]', 'y[0,0]', 'y[0,1]', 'y[1,0]', 'y[1,1]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6503/1976515759.py:4: DeprecationWarning: Model.display() is deprecated\n",
      "  model.display()\n"
     ]
    }
   ],
   "source": [
    "# Optimize the model\n",
    "model.optimize()\n",
    "\n",
    "model.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "54a6825c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimization successful!\n",
      "Optimal X values:\n",
      "X[0,0,0] * 100 = <gurobi.Var x[0,0,0] (value 1.0)>\n",
      "X[0,0,1] * 100 = <gurobi.Var x[0,0,1] (value 1.0)>\n",
      "X[0,1,0] * 1000 = <gurobi.Var x[0,1,0] (value 0.0)>\n",
      "X[0,1,1] * 1000 = <gurobi.Var x[0,1,1] (value 0.0)>\n",
      "\n",
      "Optimal Y values:\n",
      "Y[0,0] = <gurobi.Var y[0,0] (value 1.0)>\n",
      "Y[0,1] = <gurobi.Var y[0,1] (value 1.0)>\n",
      "Y[1,0] = <gurobi.Var y[1,0] (value 1.0)>\n",
      "Y[1,1] = <gurobi.Var y[1,1] (value 1.0)>\n",
      "\n",
      "Optimal objective value: 150000.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# results\n",
    "if model.status == gp.GRB.OPTIMAL:\n",
    "    print(\"\\nOptimization successful!\")\n",
    "    print(\"Optimal X values:\")\n",
    "    for e in endpoint_index:\n",
    "        for s in server_index:\n",
    "            for v in video_index:\n",
    "                print(f\"X[{e},{s},{v}] * {latency[e][s]} = {x[e,s,v]}\")\n",
    "    print(\"\\nOptimal Y values:\")\n",
    "    for s in server_index:\n",
    "        for v in video_index:\n",
    "            print(f\"Y[{s},{v}] = {y[s,v]}\")\n",
    "\n",
    "\n",
    "    print(f\"\\nOptimal objective value: {model.objVal}\")\n",
    "elif model.status == gp.GRB.INFEASIBLE:\n",
    "    print(\"Model is infeasible.\")\n",
    "elif model.status == gp.GRB.UNBOUNDED:\n",
    "    print(\"Model is unbounded.\")\n",
    "else:\n",
    "    print(f\"Optimization ended with status {model.status}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
