{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9903b1c7",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ea507c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "from collections import defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b59b1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMAL_SOLUTION = 0 # used to evaluate gap between math model and heuristics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28210e55",
   "metadata": {},
   "source": [
    "---\n",
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6217fc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_video = 0\n",
    "num_endpoint = 0\n",
    "num_req_descriptions = 0\n",
    "num_server = 0\n",
    "\n",
    "cache_capacity = 0\n",
    "video_size = []\n",
    "\n",
    "latency = defaultdict(lambda: defaultdict(int))     # [endpoint][cache/datacenter] = latency\n",
    "reqs = defaultdict(lambda: defaultdict(int))        # [endpoint][video] = num reqs\n",
    "\n",
    "# dataset = \"dataset/videos_worth_spreading.in\"\n",
    "# dataset = \"dataset/me_at_the_zoo.in\"\n",
    "dataset = \"dataset/custom.in\"\n",
    "# dataset = \"dataset/minimal.in\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c85803ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "status = 0\n",
    "endpoint_index = 0\n",
    "num_connected_cache = 0\n",
    "with open(dataset, \"r\") as f:\n",
    "    for line_content in f:\n",
    "        line = line_content.split()\n",
    "\n",
    "        if status ==0:                                  # get counters\n",
    "            num_video = int(line[0])\n",
    "            num_endpoint = int(line[1])\n",
    "            num_req_descriptions = int(line[2])\n",
    "            num_server = int(line[3])\n",
    "            cache_capacity = int(line[4])\n",
    "            status = 1\n",
    "\n",
    "        elif status == 1:                               # get video dims\n",
    "            for size in line:\n",
    "                video_size.append(int(size))\n",
    "            status = 2\n",
    "\n",
    "        elif status == 2:                               # get datacenter latency and connected cache number\n",
    "            data_center_latency = int(line[0])\n",
    "            latency[endpoint_index][num_server] = data_center_latency\n",
    "            \n",
    "            num_connected_cache = int(line[1])\n",
    "            if not num_connected_cache:\n",
    "                endpoint_index = endpoint_index + 1\n",
    "                if endpoint_index == num_endpoint:\n",
    "                    status = 4\n",
    "            else:\n",
    "                status = 3\n",
    "        \n",
    "        elif status == 3:                                  # get cache latency\n",
    "            cache_index = int(line[0])\n",
    "            cache_latency = int(line[1])\n",
    "            latency[endpoint_index][cache_index] = cache_latency\n",
    "            \n",
    "            num_connected_cache = num_connected_cache - 1\n",
    "            if not num_connected_cache:\n",
    "                endpoint_index = endpoint_index + 1\n",
    "                if endpoint_index == num_endpoint:\n",
    "                    status = 4\n",
    "                else:\n",
    "                    status = 2\n",
    "        \n",
    "        elif status == 4:                                   # take num requests\n",
    "            video_index = int(line[0])\n",
    "            endpoint_index = int(line[1])\n",
    "            num_reqs = int(line[2])\n",
    "            reqs[endpoint_index][video_index] = num_reqs                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "861cb172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num video: 5, num endpoints 2, req descriptions 4, num cache 3, dim 100\n",
      "video sized: [50, 50, 80, 30, 110]\n",
      "latencies: defaultdict(<function <lambda> at 0x7f34ec5adb20>, {0: defaultdict(<class 'int'>, {3: 1000, 0: 100, 2: 200, 1: 300}), 1: defaultdict(<class 'int'>, {3: 500})})\n",
      "reqs: defaultdict(<function <lambda> at 0x7f34d511e660>, {0: defaultdict(<class 'int'>, {3: 1500, 4: 500, 1: 1000}), 1: defaultdict(<class 'int'>, {0: 1000})})\n"
     ]
    }
   ],
   "source": [
    "print(f\"num video: {num_video}, num endpoints {num_endpoint}, req descriptions {num_req_descriptions}, num cache {num_server}, dim {cache_capacity}\")\n",
    "print(f\"video sized: {video_size}\")\n",
    "print(f\"latencies: {latency}\")\n",
    "print(f\"reqs: {reqs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a922188b",
   "metadata": {},
   "source": [
    "---\n",
    "# Math model for Guroby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47768109",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_index = range(num_endpoint)\n",
    "server_index = range(num_server + 1) # I've modelled datacenter as last server\n",
    "video_index = range(num_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bac327f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): <gurobi.Constr *Awaiting Model Update*>,\n",
       " (0, 1): <gurobi.Constr *Awaiting Model Update*>,\n",
       " (0, 2): <gurobi.Constr *Awaiting Model Update*>,\n",
       " (1, 0): <gurobi.Constr *Awaiting Model Update*>,\n",
       " (1, 1): <gurobi.Constr *Awaiting Model Update*>,\n",
       " (1, 2): <gurobi.Constr *Awaiting Model Update*>}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gp.Model(\"YoutubeCache\")\n",
    "\n",
    "# decision vars\n",
    "x = model.addVars(endpoint_index, server_index, video_index, vtype=gp.GRB.BINARY, name=\"x\")\n",
    "y = model.addVars(server_index, video_index, vtype=gp.GRB.BINARY, name=\"y\")\n",
    "\n",
    "# objective function\n",
    "obj = gp.quicksum(latency[e][s]*reqs[e][v]*x[e,s,v] for e in endpoint_index for s in server_index for v in video_index)\n",
    "# the + y[s,v] it's used just to not let place useless video in cache server (but is not needed for this problem)\n",
    "# obj = gp.quicksum((latency[e][s]*reqs[e][v]*x[e,s,v] + y[s,v])for e in endpoint_index for s in server_index for v in video_index)\n",
    "model.setObjective(obj, GRB.MINIMIZE)\n",
    "# model.setObjective(gp.quicksum(y[s,v] for s in server_index for v in video_index), GRB.MAXIMIZE)\n",
    "\n",
    "\n",
    "# constraints\n",
    "constr = (gp.quicksum(x[e,s,v] for e in endpoint_index)  <= num_endpoint*y[s,v] for s in server_index for v in video_index )\n",
    "model.addConstrs(constr, name=\"if video v available on server s\")\n",
    "\n",
    "constr = ( gp.quicksum( x[e,s,v] for s in server_index ) == (1 if reqs[e][v] else 0) for e in endpoint_index for v in video_index ) # datacenter excluded \n",
    "# constr = ( gp.quicksum( x[e,s,v] for s in latency[e] ) == (1 if reqs[e][v] else 0) for e in endpoint_index for v in video_index ) # latency[e] directly check existring connections\n",
    "model.addConstrs(constr, name=\"every request must be satisfied\")\n",
    "\n",
    "constr = ( gp.quicksum(video_size[v] * y[s,v] for v in video_index) <= cache_capacity for s in server_index[:-1] ) # -1 because datacenter have all the video\n",
    "model.addConstrs(constr, name=\"cache capacity\")\n",
    "\n",
    "\n",
    "constr = ( y[num_server,v] == 1 for v in video_index ) # cache servers are from 0 to s-1, s index (num_server) is for datacenter\n",
    "model.addConstrs(constr, name=\"Datacenter have all videos\")\n",
    "\n",
    "constr = ( gp.quicksum( x[e,s,v] for v in video_index ) <= (num_video*latency[e][s]) for e in endpoint_index for s in server_index[:-1] ) # -1 because datacenter have all the video\n",
    "model.addConstrs(constr, name=\"video v must be available on server s to be selected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "61d392c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gurobi Optimizer version 12.0.1 build v12.0.1rc0 (linux64 - \"Arch Linux\")\n",
      "\n",
      "CPU model: AMD Ryzen 7 5700U with Radeon Graphics, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 8 physical cores, 16 logical processors, using up to 16 threads\n",
      "\n",
      "Optimize a model with 44 rows, 60 columns and 150 nonzeros\n",
      "Model fingerprint: 0x198f7325\n",
      "Variable types: 0 continuous, 60 integer (60 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+02]\n",
      "  Objective range  [5e+04, 2e+06]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 2e+03]\n",
      "Found heuristic solution: objective 2600000.0000\n",
      "Presolve removed 44 rows and 60 columns\n",
      "Presolve time: 0.00s\n",
      "Presolve: All rows and columns removed\n",
      "\n",
      "Explored 0 nodes (0 simplex iterations) in 0.00 seconds (0.00 work units)\n",
      "Thread count was 1 (of 16 available processors)\n",
      "\n",
      "Solution count 2: 1.25e+06 2.6e+06 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 1.250000000000e+06, best bound 1.250000000000e+06, gap 0.0000%\n"
     ]
    }
   ],
   "source": [
    "# Optimize the model\n",
    "model.optimize()\n",
    "\n",
    "# model.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "16eebf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_full_output():\n",
    "    print(\"Optimal X [endpoint, server, video] values:\")\n",
    "    for e in endpoint_index:\n",
    "        for s in server_index:\n",
    "            for v in video_index:\n",
    "                print(f\"X[{e},{s},{v}] * {latency[e][s]} = {x[e,s,v]}\")\n",
    "    print(\"\\nOptimal Y [server, video] values:\")\n",
    "    for s in server_index:\n",
    "        for v in video_index:\n",
    "            print(f\"Y[{s},{v}] = {y[s,v]}\")\n",
    "\n",
    "def print_concise_output():\n",
    "    print(\"Optimal X [endpoint, server, video] values:\")\n",
    "    for e in endpoint_index:\n",
    "        for s in server_index:\n",
    "            for v in video_index:\n",
    "                if x[e,s,v].x:\n",
    "                    print(f\"X[{e},{s},{v}] * {latency[e][s]} = {x[e,s,v]}\")\n",
    "    print(\"\\nOptimal Y [server, video] values:\")\n",
    "    for s in server_index:\n",
    "        for v in video_index:\n",
    "            if y[s,v].x:\n",
    "                print(f\"Y[{s},{v}] = {y[s,v]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "54a6825c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimization successful!\n",
      "Optimal X [endpoint, server, video] values:\n",
      "X[0,0,1] * 100 = <gurobi.Var x[0,0,1] (value 1.0)>\n",
      "X[0,0,3] * 100 = <gurobi.Var x[0,0,3] (value 1.0)>\n",
      "X[0,3,4] * 1000 = <gurobi.Var x[0,3,4] (value 1.0)>\n",
      "X[1,3,0] * 500 = <gurobi.Var x[1,3,0] (value 1.0)>\n",
      "\n",
      "Optimal Y [server, video] values:\n",
      "Y[0,1] = <gurobi.Var y[0,1] (value 1.0)>\n",
      "Y[0,3] = <gurobi.Var y[0,3] (value 1.0)>\n",
      "Y[1,1] = <gurobi.Var y[1,1] (value 1.0)>\n",
      "Y[1,3] = <gurobi.Var y[1,3] (value 1.0)>\n",
      "Y[2,1] = <gurobi.Var y[2,1] (value 1.0)>\n",
      "Y[2,3] = <gurobi.Var y[2,3] (value 1.0)>\n",
      "Y[3,0] = <gurobi.Var y[3,0] (value 1.0)>\n",
      "Y[3,1] = <gurobi.Var y[3,1] (value 1.0)>\n",
      "Y[3,2] = <gurobi.Var y[3,2] (value 1.0)>\n",
      "Y[3,3] = <gurobi.Var y[3,3] (value 1.0)>\n",
      "Y[3,4] = <gurobi.Var y[3,4] (value 1.0)>\n",
      "\n",
      "Optimal objective value: 1250000.0\n"
     ]
    }
   ],
   "source": [
    "# results\n",
    "if model.status == gp.GRB.OPTIMAL:\n",
    "    print(\"\\nOptimization successful!\")\n",
    "    # print_full_output()\n",
    "    print_concise_output()\n",
    "    print(f\"\\nOptimal objective value: {model.objVal}\")\n",
    "    OPTIMAL_SOLUTION = model.ObjVal\n",
    "elif model.status == gp.GRB.INFEASIBLE:\n",
    "    print(\"Model is infeasible.\")\n",
    "elif model.status == gp.GRB.UNBOUNDED:\n",
    "    print(\"Model is unbounded.\")\n",
    "else:\n",
    "    print(f\"Optimization ended with status {model.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f13b29c",
   "metadata": {},
   "source": [
    "---\n",
    "# heuristics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd09de88",
   "metadata": {},
   "source": [
    "## Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d547ffa",
   "metadata": {},
   "source": [
    "### place video with highest request number in nearest cache when possible and place all videos for first endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "039aa21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APPROX RESULT: 1250000.0 - GAP: 0.0% - OPTIMAL RESULT 1250000.0\n"
     ]
    }
   ],
   "source": [
    "# 1. sort by request number. use a list containing sorted indexes\n",
    "sorted_reqs = []\n",
    "E_IND = 0\n",
    "V_IND = 1\n",
    "\n",
    "for e in range(len(reqs)):\n",
    "    # Sort v indices by descending value for this e\n",
    "    sorted_vs = sorted(\n",
    "        [v for v in reqs[e] if reqs[e][v] != 0],\n",
    "        # range(len(reqs[e])),\n",
    "        key=lambda v: reqs[e][v],\n",
    "        reverse=True\n",
    "    )\n",
    "    # Add (e, v) pairs to the list\n",
    "    sorted_reqs.extend((e, v) for v in sorted_vs)\n",
    "\n",
    "# List to store sorted (e, v) pairs\n",
    "sorted_latency = defaultdict(list)\n",
    "\n",
    "for e in latency:\n",
    "    # Sort v indices by descending value for this e\n",
    "    sorted_s = sorted(\n",
    "        [s for s in latency[e] if latency[e][s] != 0],\n",
    "        key=lambda s: latency[e][s]\n",
    "    )\n",
    "    # Add (e, v) pairs to the list\n",
    "    sorted_latency[e] = sorted_s\n",
    "\n",
    "\n",
    "# use a list to keep current cache capacity (will be decreased every time a video is placed in cache)\n",
    "curr_capacity = [cache_capacity for _ in range(num_server)]\n",
    "curr_capacity.append(float('inf')) # datacenter doesn't have capacity\n",
    "# print(curr_capacity)\n",
    "\n",
    "# create vars (simil guroby, used numpy for efficiency)\n",
    "x = np.zeros((num_endpoint, (num_server+1), num_video)) # e take v from s\n",
    "y = np.zeros(((num_server+1), num_video)) # v is in s\n",
    "y[num_server, :] = 1 # datacenter keep all the videos\n",
    "\n",
    "for req in sorted_reqs:\n",
    "    # print(req)\n",
    "    req_endpoint = req[E_IND]\n",
    "    req_video = req[V_IND]\n",
    "    req_video_size = video_size[req_video]\n",
    "\n",
    "    for curr_cache_index in sorted_latency[req_endpoint]:\n",
    "        if y[curr_cache_index, req_video]:\n",
    "            x[req_endpoint, curr_cache_index, req_video] = 1\n",
    "            break\n",
    "        else:\n",
    "            if curr_capacity[curr_cache_index] > req_video_size:\n",
    "                curr_capacity[curr_cache_index] -= req_video_size\n",
    "                y[curr_cache_index, req_video] = 1\n",
    "                x[req_endpoint, curr_cache_index, req_video] = 1\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "# print(reqs)\n",
    "# print(sorted_indexes)\n",
    "# print(sorted_latency)\n",
    "# print(\"X\")\n",
    "# print(x)\n",
    "# print(\"Y\")\n",
    "# print(y)\n",
    "APPROX_RESULT = sum(latency[e][s]*reqs[e][v]*x[e,s,v] for e in endpoint_index for s in server_index for v in video_index)\n",
    "GAP = ( abs(OPTIMAL_SOLUTION - APPROX_RESULT) / OPTIMAL_SOLUTION ) * 100\n",
    "print(f\"APPROX RESULT: {APPROX_RESULT} - GAP: {GAP}% - OPTIMAL RESULT {OPTIMAL_SOLUTION}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d9e886",
   "metadata": {},
   "source": [
    "### ROUND ROBIN: place video with highest request number in nearest cache (every iteration change endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d0775a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APPROX RESULT: 1250000.0 - GAP: 0.0% - OPTIMAL RESULT 1250000.0\n"
     ]
    }
   ],
   "source": [
    "# 1. sort by request number. use a list containing sorted indexes\n",
    "sorted_reqs = defaultdict(list)\n",
    "E_IND = 0\n",
    "V_IND = 1\n",
    "\n",
    "for e in range(len(reqs)):\n",
    "    sorted_vs = sorted(\n",
    "        [v for v in reqs[e] if reqs[e][v] != 0],\n",
    "        key=lambda v: reqs[e][v],\n",
    "        reverse=True\n",
    "    )\n",
    "    sorted_reqs[e] = sorted_vs\n",
    "\n",
    "# List to store sorted (e, v) pairs\n",
    "sorted_latency = defaultdict(list)\n",
    "\n",
    "for e in latency:\n",
    "    # Sort v indices by descending value for this e\n",
    "    sorted_s = sorted(\n",
    "        [s for s in latency[e] if latency[e][s] != 0],\n",
    "        key=lambda s: latency[e][s]\n",
    "    )\n",
    "    # Add (e, v) pairs to the list\n",
    "    sorted_latency[e] = sorted_s\n",
    "\n",
    "\n",
    "# use a list to keep current cache capacity (will be decreased every time a video is placed in cache)\n",
    "curr_capacity = [cache_capacity for _ in range(num_server)]\n",
    "curr_capacity.append(float('inf')) # datacenter doesn't have capacity\n",
    "# print(curr_capacity)\n",
    "\n",
    "# create vars (simil guroby, used numpy for efficiency)\n",
    "x = np.zeros((num_endpoint, (num_server+1), num_video)) # e take v from s\n",
    "y = np.zeros(((num_server+1), num_video)) # v is in s\n",
    "y[num_server, :] = 1 # datacenter keep all the videos\n",
    "\n",
    "Done = False\n",
    "endpoints_req_index = [0 for _ in server_index]\n",
    "while not Done:\n",
    "    Done = True\n",
    "    for curr_endpoint in endpoint_index:\n",
    "        curr_endpoint_req_index = endpoints_req_index[curr_endpoint]\n",
    "        \n",
    "        if curr_endpoint_req_index < len(sorted_reqs[curr_endpoint]):\n",
    "            Done = False # There could still be reqs not satisfied other than this\n",
    "            req_video = sorted_reqs[curr_endpoint][curr_endpoint_req_index]\n",
    "            req_video_size = video_size[req_video]\n",
    "            # print(req_video)\n",
    "\n",
    "            for curr_cache_index in sorted_latency[curr_endpoint]:\n",
    "                if y[curr_cache_index, req_video]:\n",
    "                    x[curr_endpoint, curr_cache_index, req_video] = 1\n",
    "                    break\n",
    "                else:\n",
    "                    if curr_capacity[curr_cache_index] > req_video_size:\n",
    "                        curr_capacity[curr_cache_index] -= req_video_size\n",
    "                        y[curr_cache_index, req_video] = 1\n",
    "                        x[curr_endpoint, curr_cache_index, req_video] = 1\n",
    "                        break\n",
    "                    \n",
    "        endpoints_req_index[curr_endpoint] += 1\n",
    "\n",
    "# print(reqs)\n",
    "# print(sorted_indexes)\n",
    "# print(sorted_latency)\n",
    "# print(\"X\")\n",
    "# print(x)\n",
    "# print(\"Y\")\n",
    "# print(y)\n",
    "APPROX_RESULT = sum(latency[e][s]*reqs[e][v]*x[e,s,v] for e in endpoint_index for s in server_index for v in video_index)\n",
    "GAP = ( abs(OPTIMAL_SOLUTION - APPROX_RESULT) / OPTIMAL_SOLUTION ) * 100\n",
    "print(f\"APPROX RESULT: {APPROX_RESULT} - GAP: {GAP}% - OPTIMAL RESULT {OPTIMAL_SOLUTION}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87489dbd",
   "metadata": {},
   "source": [
    "### place video with highest request number in nearest cache when possible (order over both endpoint and video, practically order by absolute req. number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b3e34ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APPROX RESULT: 1250000.0 - GAP: 0.0% - OPTIMAL RESULT 1250000.0\n"
     ]
    }
   ],
   "source": [
    "# 1. sort by request number. use a list containing sorted indexes\n",
    "sorted_reqs = []\n",
    "E_IND = 0\n",
    "V_IND = 1\n",
    "\n",
    "sorted_reqs = sorted(\n",
    "    [(e, v) for e in range(len(reqs)) for v in reqs[e] if reqs[e][v] != 0],\n",
    "    key=lambda pair: reqs[pair[0]][pair[1]],\n",
    "    reverse=True\n",
    ")\n",
    "# print(sorted_reqs)\n",
    "\n",
    "# List to store sorted (e, v) pairs\n",
    "sorted_latency = defaultdict(list)\n",
    "\n",
    "for e in latency:\n",
    "    # Sort v indices by descending value for this e\n",
    "    sorted_s = sorted(\n",
    "        [s for s in latency[e] if latency[e][s] != 0],\n",
    "        key=lambda s: latency[e][s]\n",
    "    )\n",
    "    # Add (e, v) pairs to the list\n",
    "    sorted_latency[e] = sorted_s\n",
    "\n",
    "\n",
    "# use a list to keep current cache capacity (will be decreased every time a video is placed in cache)\n",
    "curr_capacity = [cache_capacity for _ in range(num_server)]\n",
    "curr_capacity.append(float('inf')) # datacenter doesn't have capacity\n",
    "# print(curr_capacity)\n",
    "\n",
    "# create vars (simil guroby, used numpy for efficiency)\n",
    "x = np.zeros((num_endpoint, (num_server+1), num_video)) # e take v from s\n",
    "y = np.zeros(((num_server+1), num_video)) # v is in s\n",
    "y[num_server, :] = 1 # datacenter keep all the videos\n",
    "\n",
    "for req_endpoint,req_video in sorted_reqs:\n",
    "    # print(req)\n",
    "    req_video_size = video_size[req_video]\n",
    "\n",
    "    for curr_cache_index in sorted_latency[req_endpoint]:\n",
    "        if y[curr_cache_index, req_video]:\n",
    "            x[req_endpoint, curr_cache_index, req_video] = 1\n",
    "            break\n",
    "        else:\n",
    "            if curr_capacity[curr_cache_index] > req_video_size:\n",
    "                curr_capacity[curr_cache_index] -= req_video_size\n",
    "                y[curr_cache_index, req_video] = 1\n",
    "                x[req_endpoint, curr_cache_index, req_video] = 1\n",
    "                break\n",
    "\n",
    "\n",
    "# print(reqs)\n",
    "# print(sorted_indexes)\n",
    "# print(sorted_latency)\n",
    "# print(\"X\")\n",
    "# print(x)\n",
    "# print(\"Y\")\n",
    "# print(y)\n",
    "APPROX_RESULT = sum(latency[e][s]*reqs[e][v]*x[e,s,v] for e in endpoint_index for s in server_index for v in video_index)\n",
    "GAP = ( abs(OPTIMAL_SOLUTION - APPROX_RESULT) / OPTIMAL_SOLUTION ) * 100\n",
    "print(f\"APPROX RESULT: {APPROX_RESULT} - GAP: {GAP}% - OPTIMAL RESULT {OPTIMAL_SOLUTION}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0dd1d5",
   "metadata": {},
   "source": [
    "### Place video ordered by popularity in best cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa52075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function <lambda> at 0x7f34d511e660>, {0: defaultdict(<class 'int'>, {3: 1500, 4: 500, 1: 1000, 0: 0, 2: 0}), 1: defaultdict(<class 'int'>, {0: 1000, 1: 0, 2: 0, 3: 0, 4: 0})})\n",
      "defaultdict(<function <lambda> at 0x7f34c1b86de0>, {3: defaultdict(<function <lambda>.<locals>.<lambda> at 0x7f34c1b8e3e0>, {3: defaultdict(<class 'int'>, {0: 1500, 1: 2, 2: 0.0013333333333333333}), 4: defaultdict(<class 'int'>, {0: 1500, 1: 2, 2: 0.0013333333333333333}), 1: defaultdict(<class 'int'>, {0: 1500, 1: 2, 2: 0.0013333333333333333}), 0: defaultdict(<class 'int'>, {0: 1500, 1: 2, 2: 0.0013333333333333333}), 2: defaultdict(<class 'int'>, {0: 1500, 1: 2, 2: 0.0013333333333333333})}), 0: defaultdict(<function <lambda>.<locals>.<lambda> at 0x7f34c1b8eca0>, {3: defaultdict(<class 'int'>, {0: 100, 1: 1, 2: 0.01}), 4: defaultdict(<class 'int'>, {0: 100, 1: 1, 2: 0.01}), 1: defaultdict(<class 'int'>, {0: 100, 1: 1, 2: 0.01}), 0: defaultdict(<class 'int'>, {0: 100, 1: 1, 2: 0.01}), 2: defaultdict(<class 'int'>, {0: 100, 1: 1, 2: 0.01})}), 2: defaultdict(<function <lambda>.<locals>.<lambda> at 0x7f34c1b8c9a0>, {3: defaultdict(<class 'int'>, {0: 200, 1: 1, 2: 0.005}), 4: defaultdict(<class 'int'>, {0: 200, 1: 1, 2: 0.005}), 1: defaultdict(<class 'int'>, {0: 200, 1: 1, 2: 0.005}), 0: defaultdict(<class 'int'>, {0: 200, 1: 1, 2: 0.005}), 2: defaultdict(<class 'int'>, {0: 200, 1: 1, 2: 0.005})}), 1: defaultdict(<function <lambda>.<locals>.<lambda> at 0x7f34c1b86ac0>, {3: defaultdict(<class 'int'>, {0: 300, 1: 1, 2: 0.0033333333333333335}), 4: defaultdict(<class 'int'>, {0: 300, 1: 1, 2: 0.0033333333333333335}), 1: defaultdict(<class 'int'>, {0: 300, 1: 1, 2: 0.0033333333333333335}), 0: defaultdict(<class 'int'>, {0: 300, 1: 1, 2: 0.0033333333333333335}), 2: defaultdict(<class 'int'>, {0: 300, 1: 1, 2: 0.0033333333333333335})})})\n",
      "X\n",
      "[[[0. 0. 0. 1. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0.]]]\n",
      "Y\n",
      "[[1. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1.]]\n",
      "APPROX RESULT: 1350000.0 - GAP: 8.0% - OPTIMAL RESULT 1250000.0\n"
     ]
    }
   ],
   "source": [
    "# 1. sort by request number. use a list containing sorted indexes\n",
    "video_req_count = [0 for _ in video_index]\n",
    "latency_sum = defaultdict(lambda: defaultdict(lambda: defaultdict(int))) # used to find best cache to place a video [server][video][latency sum / num endpoint / score]\n",
    "LATENCY_INDEX = 0\n",
    "NUM_ENDPOINT_CONNECTED_INDEX = 1\n",
    "SCORE_INDEX = 2\n",
    "\n",
    "print(reqs)\n",
    "for curr_endpoint_index, endpoint_reqs in reqs.items():\n",
    "    # print(f\"endpoint_index: {curr_endpoint_index}\")\n",
    "    for curr_video_index, req_num in endpoint_reqs.items():\n",
    "        # print(f\"  Video: {curr_video_index}, Num reqs: {req_num}\")\n",
    "        video_req_count[curr_video_index] += req_num\n",
    "        for curr_server_index, lat in latency[curr_endpoint_index].items():\n",
    "            if lat:\n",
    "                latency_sum[curr_server_index][curr_video_index][LATENCY_INDEX] += lat\n",
    "                latency_sum[curr_server_index][curr_video_index][NUM_ENDPOINT_CONNECTED_INDEX] += 1\n",
    "                latency_sum[curr_server_index][curr_video_index][SCORE_INDEX] = (\n",
    "                    latency_sum[curr_server_index][curr_video_index][NUM_ENDPOINT_CONNECTED_INDEX] \n",
    "                    /\n",
    "                    latency_sum[curr_server_index][curr_video_index][LATENCY_INDEX]\n",
    "                )\n",
    "            \n",
    "sorted_video_indexes = sorted(range(num_video), key=lambda i: video_req_count[i], reverse=True)\n",
    "print(latency_sum)\n",
    "\n",
    "# use a list to keep current cache capacity (will be decreased every time a video is placed in cache)\n",
    "curr_capacity = [cache_capacity for _ in range(num_server)]\n",
    "curr_capacity.append(float('inf')) # datacenter doesn't have capacity\n",
    "# print(curr_capacity)\n",
    "\n",
    "# create vars (simil guroby, used numpy for efficiency)\n",
    "x = np.zeros((num_endpoint, (num_server+1), num_video)) # e take v from s\n",
    "y = np.zeros(((num_server+1), num_video)) # v is in s\n",
    "y[num_server, :] = 1 # datacenter keep all the videos\n",
    "\n",
    "sorted_latency = defaultdict(list)\n",
    "\n",
    "for e in latency:\n",
    "    # Sort v indices by ascending value for this e\n",
    "    sorted_s = sorted(\n",
    "        [s for s in latency[e] if latency[e][s] != 0],\n",
    "        key=lambda s: latency[e][s]\n",
    "    )\n",
    "    # Add (e, v) pairs to the list\n",
    "    sorted_latency[e] = sorted_s\n",
    "\n",
    "#TODO: check why video 0 is placed on cache 0 that is useless\n",
    "\n",
    "for curr_video_index in sorted_video_indexes:\n",
    "    for curr_server_index in sorted(\n",
    "        range(len(latency_sum)),\n",
    "        key=lambda i: latency_sum[i][curr_video_index][SCORE_INDEX],\n",
    "        reverse=True\n",
    "    ):\n",
    "        if not y[curr_server_index, curr_video_index]:\n",
    "            if curr_capacity[curr_server_index] > video_size[curr_video_index]:\n",
    "                curr_capacity[curr_server_index] -= video_size[curr_video_index]\n",
    "                y[curr_server_index, curr_video_index] = 1\n",
    "                break\n",
    "\n",
    "for req_endpoint,req_videos in reqs.items():    \n",
    "    for curr_video_index in req_videos:\n",
    "        if reqs[req_endpoint][curr_video_index]:\n",
    "            for curr_server_index in sorted_latency[req_endpoint]:\n",
    "                if y[curr_server_index, curr_video_index]:\n",
    "                    x[req_endpoint, curr_server_index, curr_video_index] = 1\n",
    "                    break\n",
    "\n",
    "print(\"X\")\n",
    "print(x)\n",
    "print(\"Y\")\n",
    "print(y)\n",
    "APPROX_RESULT = sum(latency[e][s]*reqs[e][v]*x[e,s,v] for e in endpoint_index for s in server_index for v in video_index)\n",
    "GAP = ( abs(OPTIMAL_SOLUTION - APPROX_RESULT) / OPTIMAL_SOLUTION ) * 100\n",
    "print(f\"APPROX RESULT: {APPROX_RESULT} - GAP: {GAP}% - OPTIMAL RESULT {OPTIMAL_SOLUTION}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f66908d",
   "metadata": {},
   "source": [
    "### place video based on connected endpoint most requested videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702ccb9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee62a39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e63615d",
   "metadata": {},
   "source": [
    "## Local Search (TODO: try tabu list, try removing cache duplicates, try increasing objective swapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7701ef4",
   "metadata": {},
   "source": [
    "## Genetic algorithm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
